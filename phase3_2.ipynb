{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "77a484dfb023468bb20a03fa9165b093": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0dcd0d0b74a844329926e60809343406",
              "IPY_MODEL_4342674a3395469b9661c4c615f3a610",
              "IPY_MODEL_7d316fbd9fb844e0bdc40e0af94a5958"
            ],
            "layout": "IPY_MODEL_31e272d1287e4321a88579553d85c05d"
          }
        },
        "0dcd0d0b74a844329926e60809343406": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_08a85c1755ad48c58d7647554d033356",
            "placeholder": "​",
            "style": "IPY_MODEL_49401d8672184effb86998f522b4a2a2",
            "value": "100%"
          }
        },
        "4342674a3395469b9661c4c615f3a610": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4b231a46bc61423ab644ddefc1dc599e",
            "max": 553433881,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4c88bff05b03437393331c9fa7af7227",
            "value": 553433881
          }
        },
        "7d316fbd9fb844e0bdc40e0af94a5958": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c37cd55a9f614a648099ea338062621c",
            "placeholder": "​",
            "style": "IPY_MODEL_33527760575b4c8c8f771b3d17fe1723",
            "value": " 528M/528M [00:06&lt;00:00, 86.2MB/s]"
          }
        },
        "31e272d1287e4321a88579553d85c05d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08a85c1755ad48c58d7647554d033356": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49401d8672184effb86998f522b4a2a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4b231a46bc61423ab644ddefc1dc599e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c88bff05b03437393331c9fa7af7227": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c37cd55a9f614a648099ea338062621c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33527760575b4c8c8f771b3d17fe1723": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejmh2DnZMIZM",
        "outputId": "12af69bb-2f19-49f3-8682-ccacc1f13b33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MSCTD'...\n",
            "remote: Enumerating objects: 1217, done.\u001b[K\n",
            "remote: Counting objects: 100% (27/27), done.\u001b[K\n",
            "remote: Compressing objects: 100% (23/23), done.\u001b[K\n",
            "remote: Total 1217 (delta 13), reused 7 (delta 3), pack-reused 1190\u001b[K\n",
            "Receiving objects: 100% (1217/1217), 102.24 MiB | 12.30 MiB/s, done.\n",
            "Resolving deltas: 100% (616/616), done.\n",
            "Updating files: 100% (934/934), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/XL2248/MSCTD.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Wr_dHRbPgrQ",
        "outputId": "5dc0c69e-b150-40af-afee-442cffb509e2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q \"/content/drive/MyDrive/Colab_Notebooks/Deep_Learning/project/train_ende.zip\"\n",
        "!unzip -q \"/content/drive/MyDrive/Colab_Notebooks/Deep_Learning/project/test.zip\"\n",
        "!unzip -q \"/content/drive/MyDrive/Colab_Notebooks/Deep_Learning/project/dev.zip\""
      ],
      "metadata": {
        "id": "6RX7stPIPiaP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import packages\n",
        "import os\n",
        "from torchvision.transforms import ToTensor, Lambda\n",
        "from torch.utils.data import Dataset\n",
        "import cv2 as cv\n",
        "import cv2\n",
        "import dlib\n",
        "import re\n",
        "from itertools import groupby\n",
        "import pandas\n",
        "from collections import Counter\n",
        "import glob\n",
        "import os.path as osp\n",
        "import random\n",
        "import numpy as np\n",
        "import json\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import torchvision\n",
        "from torchsummary import summary \n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as T\n",
        "from torch.nn.modules.container import Sequential\n",
        "from torch.utils.data import Dataset, TensorDataset, DataLoader, ConcatDataset\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import time\n",
        "from __future__ import print_function, division\n",
        "import copy\n",
        "import pathlib\n",
        "from collections import Counter\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "%matplotlib inline\n",
        "from sklearn import metrics\n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import pickle\n",
        "\n",
        "import torchtext\n",
        "from torchtext.data import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "import os.path\n",
        "import torch.nn.functional as F\n",
        "import sys\n",
        "sys.path.append('/opt/cocoapi/PythonAPI')\n",
        "from pycocotools.coco import COCO\n",
        "import math\n",
        "import re\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mi0HH-14PkY_",
        "outputId": "f3a12bb5-0841-42e9-aaa7-4c052ec74002"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (3.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk) (7.1.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "------------------------\n",
        "\n",
        "english_dev : 5063\n",
        "\n",
        "english_test : 5067\n",
        "\n",
        "english_train : 20240\n",
        "\n",
        "------------------"
      ],
      "metadata": {
        "id": "hxDDOPpXPzif"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Image Model"
      ],
      "metadata": {
        "id": "upX33Ulp3Lsb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ToNumpy = lambda X=None : np.array(X)\n",
        "y_h = lambda y : torch.zeros(3, dtype=torch.float).scatter_(dim=0, index=torch.tensor(y), value=1)\n",
        "class MSCTDDataset(Dataset):\n",
        "\n",
        "    def __init__(self, path_to_dataset = None, transform=None, target_transform=None):\n",
        "        if path_to_dataset == 'path_to_dataset_train':\n",
        "            Data = 'train'\n",
        "            path_english_text = 'english_train'\n",
        "            path_sentiment_text = 'sentiment_train'\n",
        "            path_image = 'train_ende'\n",
        "            path_image_index = \"image_index_train\"\n",
        "        elif path_to_dataset == 'path_to_dataset_test':\n",
        "            Data = 'test'\n",
        "            path_download = 'test_ende'\n",
        "            path_english_text = 'english_test'\n",
        "            path_sentiment_text = 'sentiment_test'\n",
        "            path_image = 'test'\n",
        "            path_image_index = \"image_index_test\"\n",
        "        elif path_to_dataset == 'path_to_dataset_dev':\n",
        "            Data = 'dev'\n",
        "            path_download = 'dev'\n",
        "            path_english_text = 'english_dev'\n",
        "            path_sentiment_text = 'sentiment_dev'\n",
        "            path_image = 'dev'\n",
        "            path_image_index = \"image_index_dev\"\n",
        "        else:\n",
        "            return TypeError, path_to_dataset + \" is invalid\"\n",
        "        #!unzip \"/content/drive/MyDrive/Colab_Notebooks/Deep_Learning/deepproject/train_ende.zip\"\n",
        "        english_text_path = f\"/content/MSCTD/MSCTD_data/ende/{path_english_text}.txt\"\n",
        "        sentiment_path = f\"/content/MSCTD/MSCTD_data/ende/{path_sentiment_text}.txt\"\n",
        "        image_index_path = f\"/content/MSCTD/MSCTD_data/ende/{path_image_index}.txt\"\n",
        "        self.image_path = f\"/content/{path_image}/\"\n",
        "\n",
        "        with open(english_text_path) as f:\n",
        "          english_text = [line.strip() for line in f.readlines()]\n",
        "        print(f\"{Data} english_text\", len(english_text))\n",
        "        self.englishtext = english_text\n",
        "\n",
        "        with open(sentiment_path) as f:\n",
        "          sentiment = [line.strip() for line in f.readlines()]\n",
        "        self.sentiment = sentiment\n",
        "        print(f\"{Data} sentiment\", len(sentiment))\n",
        "        \n",
        "\n",
        "        with open(image_index_path) as f:\n",
        "          indexes = [line.strip() for line in f.readlines()]\n",
        "        conv_index = []\n",
        "        for lst in indexes:\n",
        "          elements = re.findall(r'\\d+', lst)\n",
        "          res = [int(x) for x in elements]\n",
        "          conv_index.append(res)\n",
        "        self.conv_index = conv_index\n",
        "        print(f\"number of {Data} conversations\", len(self.conv_index))\n",
        "        # print(self.conv_index)\n",
        "        Images = os.listdir(self.image_path)\n",
        "        Images.sort(key = lambda x:  int(x.split(\".\")[0]))\n",
        "        self.Images = Images\n",
        "        # self.target = target\n",
        "        # self.features = features\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "    def __len__(self):\n",
        "        return len(self.sentiment)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        conversation_index = -1\n",
        "        for i in range(len(self.conv_index)):\n",
        "          if idx in self.conv_index[i]:\n",
        "            conversation_index = i+1\n",
        "            break\n",
        "        img_path = os.path.join(self.image_path, self.Images[idx])\n",
        "\n",
        "        img = cv.imread(img_path)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)   # Chnage channel due to imread function read images in BGR coding.\n",
        "        sntmnt = self.sentiment[idx]\n",
        "        englsh_txt = self.englishtext[idx]\n",
        "\n",
        "        if self.target_transform:\n",
        "            label = y_h(sntmnt)\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, englsh_txt, sntmnt, conversation_index"
      ],
      "metadata": {
        "id": "jniF8fBaPwoW"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVfiFu_kP5f8",
        "outputId": "59ed1adf-a185-47b3-ef67-d7efe38edd4d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Image Model"
      ],
      "metadata": {
        "id": "DUX0O_mAQo9-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "use_pretrained = True\n",
        "vgg16_pretrained = models.vgg16(pretrained=use_pretrained).to(device)\n",
        "summary(vgg16_pretrained, (3, 64, 64))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "77a484dfb023468bb20a03fa9165b093",
            "0dcd0d0b74a844329926e60809343406",
            "4342674a3395469b9661c4c615f3a610",
            "7d316fbd9fb844e0bdc40e0af94a5958",
            "31e272d1287e4321a88579553d85c05d",
            "08a85c1755ad48c58d7647554d033356",
            "49401d8672184effb86998f522b4a2a2",
            "4b231a46bc61423ab644ddefc1dc599e",
            "4c88bff05b03437393331c9fa7af7227",
            "c37cd55a9f614a648099ea338062621c",
            "33527760575b4c8c8f771b3d17fe1723"
          ]
        },
        "id": "n18QmuWdP8Oq",
        "outputId": "e609192e-5da0-481d-a5d2-6c84c6fc3350"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/528M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "77a484dfb023468bb20a03fa9165b093"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 64, 64]           1,792\n",
            "              ReLU-2           [-1, 64, 64, 64]               0\n",
            "            Conv2d-3           [-1, 64, 64, 64]          36,928\n",
            "              ReLU-4           [-1, 64, 64, 64]               0\n",
            "         MaxPool2d-5           [-1, 64, 32, 32]               0\n",
            "            Conv2d-6          [-1, 128, 32, 32]          73,856\n",
            "              ReLU-7          [-1, 128, 32, 32]               0\n",
            "            Conv2d-8          [-1, 128, 32, 32]         147,584\n",
            "              ReLU-9          [-1, 128, 32, 32]               0\n",
            "        MaxPool2d-10          [-1, 128, 16, 16]               0\n",
            "           Conv2d-11          [-1, 256, 16, 16]         295,168\n",
            "             ReLU-12          [-1, 256, 16, 16]               0\n",
            "           Conv2d-13          [-1, 256, 16, 16]         590,080\n",
            "             ReLU-14          [-1, 256, 16, 16]               0\n",
            "           Conv2d-15          [-1, 256, 16, 16]         590,080\n",
            "             ReLU-16          [-1, 256, 16, 16]               0\n",
            "        MaxPool2d-17            [-1, 256, 8, 8]               0\n",
            "           Conv2d-18            [-1, 512, 8, 8]       1,180,160\n",
            "             ReLU-19            [-1, 512, 8, 8]               0\n",
            "           Conv2d-20            [-1, 512, 8, 8]       2,359,808\n",
            "             ReLU-21            [-1, 512, 8, 8]               0\n",
            "           Conv2d-22            [-1, 512, 8, 8]       2,359,808\n",
            "             ReLU-23            [-1, 512, 8, 8]               0\n",
            "        MaxPool2d-24            [-1, 512, 4, 4]               0\n",
            "           Conv2d-25            [-1, 512, 4, 4]       2,359,808\n",
            "             ReLU-26            [-1, 512, 4, 4]               0\n",
            "           Conv2d-27            [-1, 512, 4, 4]       2,359,808\n",
            "             ReLU-28            [-1, 512, 4, 4]               0\n",
            "           Conv2d-29            [-1, 512, 4, 4]       2,359,808\n",
            "             ReLU-30            [-1, 512, 4, 4]               0\n",
            "        MaxPool2d-31            [-1, 512, 2, 2]               0\n",
            "AdaptiveAvgPool2d-32            [-1, 512, 7, 7]               0\n",
            "           Linear-33                 [-1, 4096]     102,764,544\n",
            "             ReLU-34                 [-1, 4096]               0\n",
            "          Dropout-35                 [-1, 4096]               0\n",
            "           Linear-36                 [-1, 4096]      16,781,312\n",
            "             ReLU-37                 [-1, 4096]               0\n",
            "          Dropout-38                 [-1, 4096]               0\n",
            "           Linear-39                 [-1, 1000]       4,097,000\n",
            "================================================================\n",
            "Total params: 138,357,544\n",
            "Trainable params: 138,357,544\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.05\n",
            "Forward/backward pass size (MB): 18.21\n",
            "Params size (MB): 527.79\n",
            "Estimated Total Size (MB): 546.05\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.modules.container import Sequential\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.features = vgg16_pretrained.features\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(output_size =(2,2))\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(in_features=2048, out_features=128, bias=True),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=0.5, inplace=False),\n",
        "            nn.Linear(in_features=128, out_features=16, bias=True),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=0.5, inplace=False),\n",
        "            nn.Linear(in_features=16, out_features=3, bias=True))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        y = self.classifier(x)\n",
        "        return y"
      ],
      "metadata": {
        "id": "6xEsGhJXQIvC"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train\n",
        "def train(address, early_stoping, patience, device, model, epochs, optimizer, loss_function, train_loader, valid_loader):\n",
        "    size = len(train_loader.dataset)\n",
        "    # Early stopping\n",
        "    last_loss = 100\n",
        "    last_acc = 0\n",
        "    triggertimes = 0\n",
        "\n",
        "    for epoch in range(1, epochs+1):\n",
        "        model.train()\n",
        "        correct = 0\n",
        "        for times, (X, txt , sntmnt, dilog) in enumerate(train_loader, 1):\n",
        "            y = []\n",
        "            X = X.to(device)\n",
        "            for lst in sntmnt:\n",
        "              elements = re.findall(r'\\d+', lst)\n",
        "              res = [int(x) for x in elements]\n",
        "              y.append(res[0])\n",
        "            y = torch.tensor(y).to(device)\n",
        "            \n",
        "            # Zero the gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward and backward propagation\n",
        "            pred = model(X)\n",
        "            loss = loss_function(pred, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "            \n",
        "            # Show progress\n",
        "            if times % 100 == 0 or times == len(train_loader):\n",
        "                print('[{}/{}, {}/{}] loss: {:.8}'.format(epoch, epochs, times, len(train_loader), loss.item()))\n",
        "\n",
        "        # Early stopping\n",
        "        correct /= size\n",
        "        current_loss, current_acc = validation(model, device, valid_loader, loss_function)\n",
        "        print('[{}/{}]'.format(epoch, epochs))\n",
        "        print('train loss: {:.8}   |||   train accuracy: {:.8}'.format(loss.item(), 100*correct))\n",
        "        print('  dev loss: {:.8}   |||     dev accuracy: {:.8}'.format(current_loss, current_acc))\n",
        "\n",
        "        if early_stoping == \"loss\":\n",
        "          C = current_loss > last_loss\n",
        "        elif early_stoping == \"accuracy\":\n",
        "          C = current_acc < last_acc\n",
        "        if C:\n",
        "            trigger_times += 1\n",
        "            print('Trigger Times:', trigger_times)\n",
        "            if trigger_times >= patience:\n",
        "                print('Early stopping!\\nStart to test process.')\n",
        "                return print(15*\"=\")\n",
        "        else:\n",
        "            torch.save(model, address)\n",
        "            print('trigger times: 0')\n",
        "            trigger_times = 0\n",
        "            if early_stoping == \"accuracy\":\n",
        "              last_acc = current_acc\n",
        "            elif early_stoping == \"loss\":\n",
        "              last_loss = current_loss\n",
        "    best_model = model = torch.load(address)\n",
        "    return print(15*\"=\")\n",
        "\n",
        "\n",
        "def validation(model, device, dataloader, loss_fn):\n",
        "    model.eval()\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for (X, txt , sntmnt, dilog) in dataloader:\n",
        "          X = X.to(device)\n",
        "          y = []\n",
        "          for lst in sntmnt:\n",
        "            elements = re.findall(r'\\d+', lst)\n",
        "            res = [int(x) for x in elements]\n",
        "            y.append(res[0])\n",
        "          y = torch.tensor(y).to(device)\n",
        "          pred = model(X)\n",
        "          test_loss += loss_fn(pred, y).item()\n",
        "          correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    return test_loss, 100*correct\n",
        "\n",
        "def test(device, model, dataloader, loss_fn):\n",
        "    model.eval()\n",
        "    true = []\n",
        "    predicted = [] \n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for (X, txt , sntmnt, dilog) in dataloader:\n",
        "          X = X.to(device)\n",
        "          y = []\n",
        "          for lst in sntmnt:\n",
        "            elements = re.findall(r'\\d+', lst)\n",
        "            res = [int(x) for x in elements]\n",
        "            y.append(res[0])\n",
        "          \n",
        "          y = torch.tensor(y).to(device)\n",
        "          pred = model(X)\n",
        "          true.append(y)\n",
        "          predicted.append(pred)\n",
        "          test_loss += loss_fn(pred, y).item()\n",
        "          correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"test Error:  Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "    return predicted, true"
      ],
      "metadata": {
        "id": "6yXATcurQPas"
      },
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU device\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "print('Device state:', device)\n",
        "\n",
        "address = \"/content/drive/MyDrive/Colab_Notebooks/Deep_Learning/project/vgg_model1\"\n",
        "torch.manual_seed(65)\n",
        "epochs = 100\n",
        "batch_size = 64\n",
        "image_size = (64, 64)\n",
        "learning_rate = 1e-3\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "model = NeuralNetwork().to(device)\n",
        "for param in model.features.parameters():\n",
        "  param.requires_grad = False\n",
        "for name, param in model.named_parameters():\n",
        "  print(f\"Layer: {name} | Size: {param.size()} | grad : {param.requires_grad} \\n\")\n",
        "    \n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "# Transform\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Resize(size = image_size)])"
      ],
      "metadata": {
        "id": "xb-7RBAtSTq_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1dfea3f8-70f6-49af-e3ba-c005a46fe201"
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device state: cuda:0\n",
            "Layer: features.0.weight | Size: torch.Size([64, 3, 3, 3]) | grad : False \n",
            "\n",
            "Layer: features.0.bias | Size: torch.Size([64]) | grad : False \n",
            "\n",
            "Layer: features.2.weight | Size: torch.Size([64, 64, 3, 3]) | grad : False \n",
            "\n",
            "Layer: features.2.bias | Size: torch.Size([64]) | grad : False \n",
            "\n",
            "Layer: features.5.weight | Size: torch.Size([128, 64, 3, 3]) | grad : False \n",
            "\n",
            "Layer: features.5.bias | Size: torch.Size([128]) | grad : False \n",
            "\n",
            "Layer: features.7.weight | Size: torch.Size([128, 128, 3, 3]) | grad : False \n",
            "\n",
            "Layer: features.7.bias | Size: torch.Size([128]) | grad : False \n",
            "\n",
            "Layer: features.10.weight | Size: torch.Size([256, 128, 3, 3]) | grad : False \n",
            "\n",
            "Layer: features.10.bias | Size: torch.Size([256]) | grad : False \n",
            "\n",
            "Layer: features.12.weight | Size: torch.Size([256, 256, 3, 3]) | grad : False \n",
            "\n",
            "Layer: features.12.bias | Size: torch.Size([256]) | grad : False \n",
            "\n",
            "Layer: features.14.weight | Size: torch.Size([256, 256, 3, 3]) | grad : False \n",
            "\n",
            "Layer: features.14.bias | Size: torch.Size([256]) | grad : False \n",
            "\n",
            "Layer: features.17.weight | Size: torch.Size([512, 256, 3, 3]) | grad : False \n",
            "\n",
            "Layer: features.17.bias | Size: torch.Size([512]) | grad : False \n",
            "\n",
            "Layer: features.19.weight | Size: torch.Size([512, 512, 3, 3]) | grad : False \n",
            "\n",
            "Layer: features.19.bias | Size: torch.Size([512]) | grad : False \n",
            "\n",
            "Layer: features.21.weight | Size: torch.Size([512, 512, 3, 3]) | grad : False \n",
            "\n",
            "Layer: features.21.bias | Size: torch.Size([512]) | grad : False \n",
            "\n",
            "Layer: features.24.weight | Size: torch.Size([512, 512, 3, 3]) | grad : False \n",
            "\n",
            "Layer: features.24.bias | Size: torch.Size([512]) | grad : False \n",
            "\n",
            "Layer: features.26.weight | Size: torch.Size([512, 512, 3, 3]) | grad : False \n",
            "\n",
            "Layer: features.26.bias | Size: torch.Size([512]) | grad : False \n",
            "\n",
            "Layer: features.28.weight | Size: torch.Size([512, 512, 3, 3]) | grad : False \n",
            "\n",
            "Layer: features.28.bias | Size: torch.Size([512]) | grad : False \n",
            "\n",
            "Layer: classifier.1.weight | Size: torch.Size([128, 2048]) | grad : True \n",
            "\n",
            "Layer: classifier.1.bias | Size: torch.Size([128]) | grad : True \n",
            "\n",
            "Layer: classifier.4.weight | Size: torch.Size([16, 128]) | grad : True \n",
            "\n",
            "Layer: classifier.4.bias | Size: torch.Size([16]) | grad : True \n",
            "\n",
            "Layer: classifier.7.weight | Size: torch.Size([3, 16]) | grad : True \n",
            "\n",
            "Layer: classifier.7.bias | Size: torch.Size([3]) | grad : True \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ToNumpy = lambda X=None : np.array(X)\n",
        "y_h = lambda y : torch.zeros(3, dtype=torch.float).scatter_(dim=0, index=torch.tensor(y), value=1)\n",
        "class MSCTDDataset(Dataset):\n",
        "\n",
        "    def __init__(self, path_to_dataset = None, transform=None, target_transform=None):\n",
        "        if path_to_dataset == 'path_to_dataset_train':\n",
        "            Data = 'train'\n",
        "            path_english_text = 'english_train'\n",
        "            path_sentiment_text = 'sentiment_train'\n",
        "            path_image = 'train_ende'\n",
        "            path_image_index = \"image_index_train\"\n",
        "        elif path_to_dataset == 'path_to_dataset_test':\n",
        "            Data = 'test'\n",
        "            path_download = 'test_ende'\n",
        "            path_english_text = 'english_test'\n",
        "            path_sentiment_text = 'sentiment_test'\n",
        "            path_image = 'test'\n",
        "            path_image_index = \"image_index_test\"\n",
        "        elif path_to_dataset == 'path_to_dataset_dev':\n",
        "            Data = 'dev'\n",
        "            path_download = 'dev'\n",
        "            path_english_text = 'english_dev'\n",
        "            path_sentiment_text = 'sentiment_dev'\n",
        "            path_image = 'dev'\n",
        "            path_image_index = \"image_index_dev\"\n",
        "        else:\n",
        "            return TypeError, path_to_dataset + \" is invalid\"\n",
        "        #!unzip \"/content/drive/MyDrive/Colab_Notebooks/Deep_Learning/deepproject/train_ende.zip\"\n",
        "        english_text_path = f\"/content/MSCTD/MSCTD_data/ende/{path_english_text}.txt\"\n",
        "        sentiment_path = f\"/content/MSCTD/MSCTD_data/ende/{path_sentiment_text}.txt\"\n",
        "        image_index_path = f\"/content/MSCTD/MSCTD_data/ende/{path_image_index}.txt\"\n",
        "        self.image_path = f\"/content/{path_image}/\"\n",
        "\n",
        "        with open(english_text_path) as f:\n",
        "          english_text = [line.strip() for line in f.readlines()]\n",
        "        print(f\"{Data} english_text\", len(english_text))\n",
        "        self.englishtext = english_text\n",
        "\n",
        "        with open(sentiment_path) as f:\n",
        "          sentiment = [line.strip() for line in f.readlines()]\n",
        "        self.sentiment = sentiment\n",
        "        print(f\"{Data} sentiment\", len(sentiment))\n",
        "        \n",
        "\n",
        "        with open(image_index_path) as f:\n",
        "          indexes = [line.strip() for line in f.readlines()]\n",
        "        conv_index = []\n",
        "        for lst in indexes:\n",
        "          elements = re.findall(r'\\d+', lst)\n",
        "          res = [int(x) for x in elements]\n",
        "          conv_index.append(res)\n",
        "        self.conv_index = conv_index\n",
        "        print(f\"number of {Data} conversations\", len(self.conv_index))\n",
        "        # print(self.conv_index)\n",
        "        Images = os.listdir(self.image_path)\n",
        "        Images.sort(key = lambda x:  int(x.split(\".\")[0]))\n",
        "        self.Images = Images\n",
        "        # self.target = target\n",
        "        # self.features = features\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "    def __len__(self):\n",
        "        return len(self.sentiment)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        conversation_index = -1\n",
        "        for i in range(len(self.conv_index)):\n",
        "          if idx in self.conv_index[i]:\n",
        "            conversation_index = i+1\n",
        "            break\n",
        "        img_path = os.path.join(self.image_path, self.Images[idx])\n",
        "\n",
        "        img = cv.imread(img_path)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)   # Chnage channel due to imread function read images in BGR coding.\n",
        "        sntmnt = self.sentiment[idx]\n",
        "        englsh_txt = self.englishtext[idx]\n",
        "\n",
        "        if self.target_transform:\n",
        "            label = y_h(sntmnt)\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, englsh_txt, sntmnt, conversation_index"
      ],
      "metadata": {
        "id": "7i9xLd-Q3bWd"
      },
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data\n",
        "training_data = MSCTDDataset('path_to_dataset_train', target_transform = False, transform = transform)\n",
        "test_data = MSCTDDataset('path_to_dataset_test', target_transform = False, transform = transform)\n",
        "dev_data = MSCTDDataset('path_to_dataset_dev', target_transform = False, transform = transform)\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
        "test_dataloader = DataLoader(test_data, batch_size=1, shuffle=True)\n",
        "dev_dataloader = DataLoader(dev_data, batch_size=batch_size, shuffle=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGCfCKhLSZxU",
        "outputId": "d4f79481-1570-4335-9388-6506127862d2"
      },
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train english_text 20240\n",
            "train sentiment 20240\n",
            "number of train conversations 2066\n",
            "test english_text 5067\n",
            "test sentiment 5067\n",
            "number of test conversations 509\n",
            "dev english_text 5063\n",
            "dev sentiment 5063\n",
            "number of dev conversations 504\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train\n",
        "print(\"\\n\\nTraining...\")\n",
        "\n",
        "early_stoping = \"accuracy\"\n",
        "patience = 2\n",
        "train(address, early_stoping, patience, device, model, epochs, optimizer, loss_fn, train_dataloader, dev_dataloader)\n",
        "best_model = torch.load(address)"
      ],
      "metadata": {
        "id": "obdwLGKNQaKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test\n",
        "predicted, true = test(device, best_model, test_dataloader, loss_fn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBMRDryCKkjw",
        "outputId": "5b7ebccd-9df1-4acd-8607-8f305e7f6b79"
      },
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test Error:  Accuracy: 40.8%, Avg loss: 1.095211 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "true"
      ],
      "metadata": {
        "id": "fuvhIiiqLRG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_tr = [i.item() for i in true]\n",
        "y_pr = [torch.argmax(i[0]).item() for i in predicted]"
      ],
      "metadata": {
        "id": "eLby7NX7K4dU"
      },
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "print(classification_report(y_tr, y_pr))\n",
        "print(confusion_matrix(y_tr, y_pr))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qMY1WjjK20d",
        "outputId": "d0391bc1-b170-448f-f970-d5c5c6473842"
      },
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.26      0.16      0.20      1298\n",
            "           1       0.44      0.86      0.58      2163\n",
            "           2       0.00      0.00      0.00      1606\n",
            "\n",
            "    accuracy                           0.41      5067\n",
            "   macro avg       0.23      0.34      0.26      5067\n",
            "weighted avg       0.25      0.41      0.30      5067\n",
            "\n",
            "[[ 211 1087    0]\n",
            " [ 305 1858    0]\n",
            " [ 284 1322    0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = torch.load(\"/content/drive/MyDrive/Colab_Notebooks/Deep_Learning/project/Parvizi_Part2\", map_location=torch.device('cuda'))\n",
        "\n",
        "# Test\n",
        "predicted, true = test(device, best_model, test_dataloader, loss_fn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OXPNwquSIYm",
        "outputId": "2551a34c-36f6-4ad9-b22e-0d6f407a3985"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test Error:  Accuracy: 41.1%, Avg loss: 1.094047 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_tr = [i.item() for i in true]\n",
        "y_pr = [torch.argmax(i[0]).item() for i in predicted]"
      ],
      "metadata": {
        "id": "sIDpTdb5vLdI"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "print(classification_report(y_tr, y_pr))\n",
        "print(confusion_matrix(y_tr, y_pr))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAcgl0BWsSBI",
        "outputId": "d4a28ada-3ebd-487c-a84d-ea1ec1fa513e"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.28      0.14      0.19      1298\n",
            "           1       0.43      0.88      0.58      2163\n",
            "           2       0.00      0.00      0.00      1606\n",
            "\n",
            "    accuracy                           0.41      5067\n",
            "   macro avg       0.24      0.34      0.26      5067\n",
            "weighted avg       0.26      0.41      0.30      5067\n",
            "\n",
            "[[ 188 1110    0]\n",
            " [ 267 1896    0]\n",
            " [ 221 1385    0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Text Model"
      ],
      "metadata": {
        "id": "_mVq13Vjv_De"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 68\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "from torchtext.data import get_tokenizer\n",
        "# from sklearn.model_selection import train_test_split\n",
        "from torchtext.vocab import build_vocab_from_iterator"
      ],
      "metadata": {
        "id": "I_ryNTBzaA86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_h = lambda y : torch.zeros(3, dtype=torch.float).scatter_(dim=0, index=torch.tensor(y), value=1)\n",
        "class MSCTDDataset(Dataset):\n",
        "\n",
        "    def __init__(self, path_to_dataset = None, transform=None, target_transform=None):\n",
        "        if path_to_dataset == 'path_to_dataset_train':\n",
        "            Data = 'train'\n",
        "            path_english_text = 'english_train'\n",
        "            path_sentiment_text = 'sentiment_train'\n",
        "            path_image = 'train_ende'\n",
        "            path_image_index = \"image_index_train\"\n",
        "        elif path_to_dataset == 'path_to_dataset_test':\n",
        "            Data = 'test'\n",
        "            path_download = 'test_ende'\n",
        "            path_english_text = 'english_test'\n",
        "            path_sentiment_text = 'sentiment_test'\n",
        "            path_image = 'test'\n",
        "            path_image_index = \"image_index_test\"\n",
        "        elif path_to_dataset == 'path_to_dataset_dev':\n",
        "            Data = 'dev'\n",
        "            path_download = 'dev'\n",
        "            path_english_text = 'english_dev'\n",
        "            path_sentiment_text = 'sentiment_dev'\n",
        "            path_image = 'dev'\n",
        "            path_image_index = \"image_index_dev\"\n",
        "        else:\n",
        "            return TypeError, path_to_dataset + \" is invalid\"\n",
        "        #!unzip \"/content/drive/MyDrive/Colab_Notebooks/Deep_Learning/deepproject/train_ende.zip\"\n",
        "        english_text_path = f\"/content/MSCTD/MSCTD_data/ende/{path_english_text}.txt\"\n",
        "        sentiment_path = f\"/content/MSCTD/MSCTD_data/ende/{path_sentiment_text}.txt\"\n",
        "        image_index_path = f\"/content/MSCTD/MSCTD_data/ende/{path_image_index}.txt\"\n",
        "        self.image_path = f\"/content/{path_image}/\"\n",
        "\n",
        "        with open(english_text_path) as f:\n",
        "          english_text = [line.strip() for line in f.readlines()]\n",
        "        print(f\"{Data} english_text\", len(english_text))\n",
        "        self.englishtext = english_text\n",
        "\n",
        "        with open(sentiment_path) as f:\n",
        "          sentiment = [line.strip() for line in f.readlines()]\n",
        "        self.sentiment = sentiment\n",
        "        print(f\"{Data} sentiment\", len(sentiment))\n",
        "        with open(image_index_path) as f:\n",
        "          indexes = [line.strip() for line in f.readlines()]\n",
        "        conv_index = []\n",
        "        for lst in indexes:\n",
        "          elements = re.findall(r'\\d+', lst)\n",
        "          res = [int(x) for x in elements]\n",
        "          conv_index.append(res)\n",
        "        self.conv_index = conv_index\n",
        "        print(f\"number of {Data} conversations\", len(self.conv_index))\n",
        "        Images = os.listdir(self.image_path)\n",
        "        Images.sort(key = lambda x:  int(x.split(\".\")[0]))\n",
        "        self.Images = Images\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "        transform = transforms.Compose([transforms.ToTensor()])\n",
        "    def __len__(self):\n",
        "        return len(self.sentiment)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        conversation_index = -1\n",
        "        for i in range(len(self.conv_index)):\n",
        "          if idx in self.conv_index[i]:\n",
        "            conversation_index = i+1\n",
        "            break\n",
        "        sntmnt = self.sentiment[idx]\n",
        "        englsh_txt = self.englishtext[idx]\n",
        "\n",
        "        if self.target_transform:\n",
        "            label = y_h(sntmnt)\n",
        "        return englsh_txt, sntmnt, conversation_index"
      ],
      "metadata": {
        "id": "XNnZek_lXd3N"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ = MSCTDDataset('path_to_dataset_train', target_transform = False)\n",
        "test_ = MSCTDDataset('path_to_dataset_test', target_transform = False)\n",
        "dev_ = MSCTDDataset('path_to_dataset_dev', target_transform = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UuvCEllJSdky",
        "outputId": "ed93ee4e-e759-4965-bd7f-7a7536ac81b4"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train english_text 20240\n",
            "train sentiment 20240\n",
            "number of train conversations 2066\n",
            "test english_text 5067\n",
            "test sentiment 5067\n",
            "number of test conversations 509\n",
            "dev english_text 5063\n",
            "dev sentiment 5063\n",
            "number of dev conversations 504\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eng_list=[x[0] for x in train_]\n",
        "lable_list=[x[1] for x in train_]\n",
        "train_label = [eval(i) for i in lable_list]\n",
        "cnvr_list=[x[2] for x in train_]"
      ],
      "metadata": {
        "id": "FMx_tm4VXQwf"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(list(set(cnvr_list)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9Xd5XZAaIHH",
        "outputId": "152ada5d-2743-45bc-9b50-ab8e10e257a1"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2066"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "def build_vocab(datasets):\n",
        "  for _, text in datasets:\n",
        "    yield tokenizer(text)\n",
        "sentences = enumerate(eng_list)\n",
        "token_generator = build_vocab(sentences)\n",
        "vocab = build_vocab_from_iterator(token_generator,\n",
        "                                  min_freq=1,\n",
        "                                  specials=['<sos>', '<eos>', '<unk>'])\n",
        "vocab.set_default_index(vocab[\"<unk>\"])"
      ],
      "metadata": {
        "id": "99naHypbaJ6o"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"vocab length\", len(vocab))\n",
        "print(\"index of <ali>\", vocab[\"ali\"])\n",
        "print(\"index of <unk>\", vocab[\"<unk>\"])\n",
        "print(\"index of <sos>\", vocab[\"<sos>\"])\n",
        "print(\"index of <eos>\", vocab[\"<eos>\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qLoxd0RaLzP",
        "outputId": "050d9a4b-f175-491c-de13-bfb2662a12ca"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab length 9874\n",
            "index of <ali> 2\n",
            "index of <unk> 2\n",
            "index of <sos> 0\n",
            "index of <eos> 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SVM = pickle.load(open(\"/content/drive/MyDrive/Colab_Notebooks/Deep_Learning/project/embedding_vec_2.p\", \"rb\"))"
      ],
      "metadata": {
        "id": "5PFY6O4_afJk"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(SVM[\"<eos>\"])\n",
        "print(SVM[\"<sos>\"])\n",
        "print(SVM[\"<unk>\"].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89hFfEJ2a8x4",
        "outputId": "546aaace-e40a-4564-e363-43bca32aa46c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-3.34279203e-04  9.66934375e-05  9.09144688e-04 ... -4.08905359e-03\n",
            "  -9.13396576e-04  0.00000000e+00]]\n",
            "[[ 1.88680133e-03  7.24659124e-05  4.27327657e-04 ...  7.18038536e-04\n",
            "  -1.03761044e-04  0.00000000e+00]]\n",
            "(1, 2067)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize_batch(batch):\n",
        "    length = 15\n",
        "    res = [[j for j, k, l in batch], [k for j, k, l  in batch]]\n",
        "    # print(res[0])\n",
        "    X, Y = res[0], res[1]\n",
        "    Y = [eval(i) for i in Y]\n",
        "    X = [re.sub(r'[^\\w\\s]','', sntnce) for sntnce in X]\n",
        "    X = [re.sub(r'\\d+', '', sntnce) for sntnce in X]\n",
        "    X = [sntnce.lower() for sntnce in X]\n",
        "    X = [nltk.tokenize.word_tokenize(sample) for sample in X]\n",
        "    # print(X)\n",
        "    X = [vocab(sample) for sample in X]\n",
        "    # print(X)\n",
        "    X = [sample+([2]* (length-len(sample))) if len(sample) < length else sample[:length] for sample in X]\n",
        "    [x.append(1) for x in X]\n",
        "    [x.insert(0,0) for x in X]\n",
        "    return torch.tensor(X, dtype=torch.int32).transpose(0, 1).to(device), torch.tensor(Y, dtype=torch.int32).to(device)\n",
        "# train_data, test_data, val_data = to_map_style_dataset(train_data), to_map_style_dataset(test_data), to_map_style_dataset(val_data)\n",
        "train = MSCTDDataset('path_to_dataset_train', target_transform = False)\n",
        "test = MSCTDDataset('path_to_dataset_test', target_transform = False)\n",
        "dev = MSCTDDataset('path_to_dataset_dev', target_transform = False)\n",
        "\n",
        "train_loader = DataLoader(train, batch_size=512, collate_fn=vectorize_batch)\n",
        "test_loader  = DataLoader(test, batch_size=512, collate_fn=vectorize_batch)\n",
        "val_loader  = DataLoader(dev, batch_size=512, collate_fn=vectorize_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DjZQ2frfZl1b",
        "outputId": "e5739e4b-3950-4f27-c194-3eed68a083f3"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train english_text 20240\n",
            "train sentiment 20240\n",
            "number of train conversations 2066\n",
            "test english_text 5067\n",
            "test sentiment 5067\n",
            "number of test conversations 509\n",
            "dev english_text 5063\n",
            "dev sentiment 5063\n",
            "number of dev conversations 504\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BiDGRU(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, dropout):\n",
        "        super().__init__()\n",
        "        self.hid_dim = hid_dim\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.rnn = nn.GRU(emb_dim, hid_dim, bidirectional= True)\n",
        "        self.fc1 = nn.Linear(hid_dim * 2, hid_dim)\n",
        "        self.fc2 = nn.Linear(hid_dim, 3)\n",
        "        self.probs = nn.Softmax(dim=1)\n",
        "    def forward(self, src):\n",
        "        s = self.embedding(src)\n",
        "        embedded = self.dropout(s)\n",
        "        outputs, hidden = self.rnn(embedded)\n",
        "        hidden = torch.tanh(self.fc1(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
        "        hidden = self.fc2(hidden)\n",
        "        probabilities = self.probs(hidden)\n",
        "        return probabilities"
      ],
      "metadata": {
        "id": "OZuVSRArazAu"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 59\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "input_dim = len(vocab)\n",
        "output_dim = len(vocab)\n",
        "enc_emb_dim = 256\n",
        "dec_emb_dim = 256\n",
        "hid_dim = 512\n",
        "enc_dropout = 0.5\n",
        "dec_dropout = 0.5\n",
        "\n",
        "model = BiDGRU(len(vocab), 256, 512, 0.5).to(device)\n",
        "\n",
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.normal_(param.data, mean=0, std=0.01)\n",
        "        \n",
        "model.apply(init_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWRdCw2Wa2ji",
        "outputId": "8864f9ad-c5ce-4e6e-c72e-60be5ed625fb"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BiDGRU(\n",
              "  (embedding): Embedding(9874, 256)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              "  (rnn): GRU(256, 512, bidirectional=True)\n",
              "  (fc1): Linear(in_features=1024, out_features=512, bias=True)\n",
              "  (fc2): Linear(in_features=512, out_features=3, bias=True)\n",
              "  (probs): Softmax(dim=1)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(model.parameters())\n",
        "target_padding_index = vocab.get_stoi()[\"<unk>\"]\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = target_padding_index).to(device)\n",
        "criterion = nn.CrossEntropyLoss().to(device)"
      ],
      "metadata": {
        "id": "M_SobecFbBsw"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "metadata": {
        "id": "L6vMQZ_qbDbn"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    ns = 0\n",
        "    aize = len(iterator)\n",
        "    correct = 0\n",
        "    for i, batch in enumerate(iterator):\n",
        "        src, trg = batch\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src)\n",
        "        output_dim = output.shape[-1]\n",
        "        output = output[1:,].view(-1, output_dim)\n",
        "        trg = trg[1:,].reshape(output.shape[0])\n",
        "        trg = trg.type(torch.LongTensor).to(device)\n",
        "        loss = criterion(output, trg)\n",
        "        correct += (output.argmax(1) == trg).type(torch.float).sum().item()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "        ns += src.shape[1]\n",
        "        epoch_loss += loss.item()\n",
        "    correct /= ns\n",
        "    return correct, epoch_loss / len(iterator)\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "    epoch_loss = 0\n",
        "    correct = 0\n",
        "    ns = 0\n",
        "    true, predicted = [], []\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(iterator):\n",
        "\n",
        "            src, trg = batch\n",
        "            output = model(src)\n",
        "            output_dim = output.shape[-1]\n",
        "            output = output.view(-1, output_dim)\n",
        "            true.append(trg)\n",
        "            predicted.append(output)\n",
        "            trg = trg.reshape(output.shape[0])\n",
        "            trg = trg.type(torch.LongTensor).to(device)\n",
        "            loss = criterion(output, trg)\n",
        "            correct += (output.argmax(1) == trg).sum().item()\n",
        "            ns += src.shape[1]\n",
        "            epoch_loss += loss.item()\n",
        "    correct /= ns\n",
        "    return true, predicted, correct , epoch_loss / len(iterator)"
      ],
      "metadata": {
        "id": "L_5qsOnWbFCn"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N_EPOCHS = 23\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "train_history = []\n",
        "val_history = []\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time()\n",
        "\n",
        "    train_acc, train_loss = train(model, train_loader, optimizer, criterion, CLIP)\n",
        "    model.eval()\n",
        "    _,_,val_acc, valid_loss = evaluate(model, val_loader, criterion)\n",
        "    train_history += [train_loss]\n",
        "    val_history += [valid_loss]\n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f} | Train acc: {train_acc:.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f} | Val acc: {val_acc:.3f}')\n",
        "plt.plot(range(epoch+1), train_history)\n",
        "plt.plot(range(epoch+1), val_history)\n",
        "plt.plot()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "braitD9ybGca",
        "outputId": "8cc939d6-8b5c-4932-9948-548100621647"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Time: 0m 9s\n",
            "\tTrain Loss: 1.089 | Train PPL:   2.973 | Train acc: 0.400\n",
            "\t Val. Loss: 1.098 |  Val. PPL:   2.999 | Val acc: 0.370\n",
            "Epoch: 02 | Time: 0m 7s\n",
            "\tTrain Loss: 1.036 | Train PPL:   2.818 | Train acc: 0.456\n",
            "\t Val. Loss: 1.049 |  Val. PPL:   2.854 | Val acc: 0.443\n",
            "Epoch: 03 | Time: 0m 9s\n",
            "\tTrain Loss: 0.987 | Train PPL:   2.683 | Train acc: 0.518\n",
            "\t Val. Loss: 1.033 |  Val. PPL:   2.810 | Val acc: 0.470\n",
            "Epoch: 04 | Time: 0m 9s\n",
            "\tTrain Loss: 0.938 | Train PPL:   2.554 | Train acc: 0.595\n",
            "\t Val. Loss: 1.036 |  Val. PPL:   2.817 | Val acc: 0.493\n",
            "Epoch: 05 | Time: 0m 7s\n",
            "\tTrain Loss: 0.920 | Train PPL:   2.509 | Train acc: 0.619\n",
            "\t Val. Loss: 1.124 |  Val. PPL:   3.076 | Val acc: 0.417\n",
            "Epoch: 06 | Time: 0m 9s\n",
            "\tTrain Loss: 0.961 | Train PPL:   2.615 | Train acc: 0.580\n",
            "\t Val. Loss: 1.033 |  Val. PPL:   2.808 | Val acc: 0.505\n",
            "Epoch: 07 | Time: 0m 8s\n",
            "\tTrain Loss: 0.899 | Train PPL:   2.458 | Train acc: 0.640\n",
            "\t Val. Loss: 1.027 |  Val. PPL:   2.793 | Val acc: 0.510\n",
            "Epoch: 08 | Time: 0m 8s\n",
            "\tTrain Loss: 0.877 | Train PPL:   2.404 | Train acc: 0.666\n",
            "\t Val. Loss: 1.033 |  Val. PPL:   2.809 | Val acc: 0.502\n",
            "Epoch: 09 | Time: 0m 9s\n",
            "\tTrain Loss: 0.872 | Train PPL:   2.392 | Train acc: 0.672\n",
            "\t Val. Loss: 1.036 |  Val. PPL:   2.819 | Val acc: 0.505\n",
            "Epoch: 10 | Time: 0m 7s\n",
            "\tTrain Loss: 0.879 | Train PPL:   2.408 | Train acc: 0.663\n",
            "\t Val. Loss: 1.033 |  Val. PPL:   2.809 | Val acc: 0.509\n",
            "Epoch: 11 | Time: 0m 9s\n",
            "\tTrain Loss: 0.872 | Train PPL:   2.391 | Train acc: 0.671\n",
            "\t Val. Loss: 1.034 |  Val. PPL:   2.813 | Val acc: 0.507\n",
            "Epoch: 12 | Time: 0m 9s\n",
            "\tTrain Loss: 0.851 | Train PPL:   2.342 | Train acc: 0.694\n",
            "\t Val. Loss: 1.039 |  Val. PPL:   2.826 | Val acc: 0.502\n",
            "Epoch: 13 | Time: 0m 7s\n",
            "\tTrain Loss: 0.844 | Train PPL:   2.325 | Train acc: 0.702\n",
            "\t Val. Loss: 1.033 |  Val. PPL:   2.810 | Val acc: 0.507\n",
            "Epoch: 14 | Time: 0m 9s\n",
            "\tTrain Loss: 0.839 | Train PPL:   2.315 | Train acc: 0.706\n",
            "\t Val. Loss: 1.030 |  Val. PPL:   2.800 | Val acc: 0.510\n",
            "Epoch: 15 | Time: 0m 9s\n",
            "\tTrain Loss: 0.834 | Train PPL:   2.303 | Train acc: 0.712\n",
            "\t Val. Loss: 1.025 |  Val. PPL:   2.787 | Val acc: 0.515\n",
            "Epoch: 16 | Time: 0m 7s\n",
            "\tTrain Loss: 0.830 | Train PPL:   2.293 | Train acc: 0.717\n",
            "\t Val. Loss: 1.033 |  Val. PPL:   2.810 | Val acc: 0.507\n",
            "Epoch: 17 | Time: 0m 9s\n",
            "\tTrain Loss: 0.829 | Train PPL:   2.291 | Train acc: 0.717\n",
            "\t Val. Loss: 1.034 |  Val. PPL:   2.814 | Val acc: 0.502\n",
            "Epoch: 18 | Time: 0m 8s\n",
            "\tTrain Loss: 0.825 | Train PPL:   2.281 | Train acc: 0.721\n",
            "\t Val. Loss: 1.024 |  Val. PPL:   2.785 | Val acc: 0.519\n",
            "Epoch: 19 | Time: 0m 8s\n",
            "\tTrain Loss: 0.819 | Train PPL:   2.268 | Train acc: 0.728\n",
            "\t Val. Loss: 1.025 |  Val. PPL:   2.787 | Val acc: 0.518\n",
            "Epoch: 20 | Time: 0m 9s\n",
            "\tTrain Loss: 0.815 | Train PPL:   2.260 | Train acc: 0.731\n",
            "\t Val. Loss: 1.026 |  Val. PPL:   2.791 | Val acc: 0.517\n",
            "Epoch: 21 | Time: 0m 7s\n",
            "\tTrain Loss: 0.814 | Train PPL:   2.257 | Train acc: 0.732\n",
            "\t Val. Loss: 1.030 |  Val. PPL:   2.802 | Val acc: 0.514\n",
            "Epoch: 22 | Time: 0m 9s\n",
            "\tTrain Loss: 0.814 | Train PPL:   2.257 | Train acc: 0.732\n",
            "\t Val. Loss: 1.029 |  Val. PPL:   2.798 | Val acc: 0.511\n",
            "Epoch: 23 | Time: 0m 9s\n",
            "\tTrain Loss: 0.811 | Train PPL:   2.250 | Train acc: 0.735\n",
            "\t Val. Loss: 1.030 |  Val. PPL:   2.802 | Val acc: 0.514\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 117
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXxU13338c9vRvu+7yub2VexQ2w3LsZuYhzbSb0lTkJMXdtp2jRPHrdNmzxu06RPkzZJ6ziPt+DUW4ib2LixjZfYCTsIGwwCswu0IgmBEBLaz/PHuYIBJDRII82i3/v1mtfM3Htn5qdh+M6dc885V4wxKKWUCl0ufxeglFJqeGnQK6VUiNOgV0qpEKdBr5RSIU6DXimlQlyYvwu4VFpamikqKvJ3GUopFVR27NjRYIxJ72tdwAV9UVERpaWl/i5DKaWCiogc62+dNt0opVSI06BXSqkQp0GvlFIhToNeKaVCnAa9UkqFOA16pZQKcRr0SikV4jTog0VlKVRs93cVSqkgFHADplQ/Xn0IjIGHt/m7EqVUkNGgDwZn66H+Y3u7pQFi0/xbj1IqqGjTTTA4tuHC7eOb/VeHUiooadAHg/KNEB4LYdFwbJO/q1FKBRltugkG5RugYD50d2rQK6Wumu7RB7qWBqjfB0VLoHAR1H4EbWf8XZVSKoho0Ae6YxvtddFSG/SmByq1541Synsa9IGufAOEx0DOLMibC64wbb5RSl0VbaMPdOUbIH8+uMPtJXsmHNOeN0op7+kefSBrOQl1e237fK/ChVBVCp1t/qtLKRVUNOgD2fn2ec+gXwzdHVD9gX9qUkoFHQ36QFa+wfadz5l9YVn+fHvd+yWglFIDGDDoReQZEakTkT39rJ8oIptFpF1EvnHJuuUisl9EDonII74qetQ4ttH2nw+LuLAsJgUypmg7vVLKa97s0a8Gll9hfSPwF8APPBeKiBt4DLgJmAzcJSKTB1fmKNTaCCf2QOGSy9cVLoSKrdDdNfJ1KaWCzoBBb4z5AzbM+1tfZ4zZDnResmoecMgYc8QY0wG8BKwYSrGjSl/t870KF0HHWTt4SimlBjCcbfS5QIXH/Upn2WVEZJWIlIpIaX19/eBereUkvPm3UNtnC1PwKd9o2+dzZ1++rmCRvdYJzpRSXgiIg7HGmCeMMSXGmJL09PTBPYnLBTt+Dlt+6tvi/KV8A+TPhbDIy9clZENysQ6cUkp5ZTiDvgrI97if5ywbHtHJMPNu2P0rOFs3bC8zInrb54uW9r9N4SIb9MaMXF1KqaA0nEG/HRgvIsUiEgHcCawdxteD+X9u+5hvf3pYX2bYHd8MGNtnvj+Fi+BcI9TvH7GylFLByZvulS8Cm4FrRKRSRFaKyAMi8oCzPktEKoGvA99ytkkwxnQBDwPrgH3AGmNM2fD9KUDaOJiwHLY/FdwjR8s3QFgU5M7pf5uChfb6uDbfKKWubMC5bowxdw2wvhbbLNPXuteB1wdX2tXr7jEw7wHcB26FPS/DrHtH6qV9q3yDncAsPKr/bVLGQFyWbb4p+fLI1aaUCjoBcTDWF46fbGXpv/yON1omQOZU2PzT4Gy/PncKanf33a3Sk4jtT6/t9EqpAYRM0OcmR+NyCc9tPQ4L/hzqyuDo7/1d1tU75rTPDxT0YNvwz1TB6ePDXpZSKniFTNC7XcLd8wvYcqSRw5k3Qmy63asPNsc2gjsScksG3vZ8O732p1dK9S9kgh7gcyX5hLuF53bUQclKOLgOGg75u6yrU75+4Pb5XhmTISpRJzhTSl1RSAV9WlwkN03N5r93VHJuxhfBHQFbH/d3Wd47dxpqPvKu2QbsILGChTrBmVLqikIq6AHumV/AmbYuXjvcBdM+BztfsAOQgsHxLXjdPt+rcBGcPBj8g8SUUsMm5IJ+XnEKEzLjeG7rMVjwAHS2wge/8HdZ3ilfb3+F5HnRPt9L571RSg0g5IJeRLhnfiEfVTbxUVc+FH8Ctj0B3ZdOrhmAjm102uejvX9M9gx78nCd90Yp1Y+QC3qAz8zOJTrczXNbjsGCh2wXxL2v+rusK2trgppdV9dsA/akJHklGvRKqX6FZNAnRIVz66wc1u6qpinvekgZa2e1DOSBRce3gOm58vw2/SlcbAdZtTX5vi6lVNALyaAHuGd+IW2dPfx6Z7UdQFW1Ayq3+7us/pVvcNrn5179YwsXAQYqtvm8LKVU8AvZoJ+am8jM/CSe33ocM+NO299882P+Lqt/5RvsIKmImKt/bG4JuMK1P71Sqk8hG/Rgu1oeqjvLlsoOmH0f7FsbmNMFtJ2Bmp1QNIhmG7BfDjmztD+9UqpPIR30n56RQ2J0uO1qOf/PALE9cAJNxVbbPn+1B2I9FS60zVOd53xXl1IqJIR00EeFu7ljTh7r9tRS50qDyStgxy+g/ay/S7tY+Xrb9JI3b/DPUbgYejpt2CullIeQDnqwzTddPYY12ytg4UPQ3gQ7n/d3WRcr32BPMjKY9vle+fMB0W6WSqnLhHzQj0mPY/G4VF7cVkF3zhzbq2XL49DT7e/SrPZmqN45tGYbgOgkOw+/HpBVSl0i5IMe4N75hVSdPsf7++tgwYNw6igcWOfvsqzjW8F0Dz3owbbTV2wPjlHASqkRMyqC/obJmWTER9qRspNugYQ8O4AqEJSvB1cY5A+hfb5X4SLobLEzYCqllGNUBH2428Wdc/N5/0A9FU0dMH+VDdhACMRjG532+dihP9f5Cc60nV4pdcGoCHqAO+cVIMAL247bPvXhsbat3p/az0LVB75ptgGIz7TTPegBWaWUh1ET9DlJ0XxyUiZrtlfQHh4Ps+6BPS9D8wn/FVXhtM8PZn6b/hQutFMW9/T47jmVUkFt1AQ9wL0LCjnZ0sGbe2ph/gP2oOX2p/xXUPkGp31+vu+es3AxnDsF9R/77jmVUkFtwKAXkWdEpE5E9vSzXkTkJyJySEQ+EpHZHuu6RWSnc1nry8IHY+m4NApTY3h+y3FIHQsTlkPp0/4bTVq+AXJmQ2Sc757z/AnDtflGKWV5s0e/Glh+hfU3AeOdyyrAs+H7nDFmpnO5ZdBV+ojLJdw9r4Bt5Y0cONEMCx+E1pOw+1cjX0xHC1R/MPj5bfqTXATxOdpOr5Q6b8CgN8b8AbjSSVdXAL8w1hYgSUSyfVWgr322JJ8It4vntxyDoqV2kNFmP8xVX7EVerp8dyC2l4htpz+2ObDn31dKjRhftNHnAhUe9yudZQBRIlIqIltE5Nb+nkBEVjnbldbX1/ugpP6lxEZw87Qsfv1BFS0d3XYAVf0+eO1rdi97pJRvAHFD/gLfP3fhImiuhlPlvn9upVTQGe6DsYXGmBLgbuBHIjK2r42MMU8YY0qMMSXp6enDXJI9KNvc3sXaXdUw405Y/DV7AvGfLYXKEZoUrHyjnVrYl+3zvfSE4UopD74I+iog3+N+nrMMY0zv9RHgfWCWD15vyOYUJjMxK57nthzDiAv++FH44v9Adwc8/cfw/r9Ad9fwFdDRYmeZ9HWzTa/0iRCdrPPeKKUA3wT9WuALTu+bBUCTMaZGRJJFJBJARNKAxcBeH7zekIkI9ywopKz6DDsrTtuFRUvggQ0w9XZ4/5/h58uh8cjwFFCxzU4pXLR0eJ7f5bK9b/REJEopvOte+SKwGbhGRCpFZKWIPCAiDzibvA4cAQ4BTwIPOssnAaUisgt4D/i+MSYggh7gM7NyiY1w89wWjzNORSfB7U/C7U9D/QF4fIlt0vH1Qc1jG237fIEP+89fqnARNB7274AwpVRACBtoA2PMXQOsN8BDfSzfBEwbfGnDKy4yjFtn5fLyjkr+/lOTSIqJuLBy2h1QsAB+8wCs/aqd6fLTP4bYNN+8ePkGyJkJkfG+eb6+eM57M+Uzw/c6SqmAN6pGxl7qnvmFtHf18PKOystXJubBF9bCsn+Cg2/BTxfCwbeH/qIdrVBZOnzt872yp9v5fLQ//eB1d0Htbij9Obz6EDy+GFZ/Cn7/f22zWFeHvytUyisD7tGHssk5CcwuSOKFrcdZuaQYEbl4A5cLFn0VxlwH/30/PH8HzL3fHrwd7NmgKrfb9vnCYQ56dzjkzw3sdnpj4Ew1NOyHlgY7Wjl9om9m8hxULVX2IHllqb2u/hA6W+366GQ7irmlDt77Z8BAeIydvqJ4KRR9wvaico/q/1IqQI36T+W9Cwr5+ppdbDjUwNLx/XTtzJoGq96Hdx+FLY/B0d/DbU/a5pdLGWPnmmmutX3Zm2vhTM2F2yfKQFy2aWi4FS62oXTutD3+4C/dXfZkL/X7bajXH7DXDQeho4/z9yYVQsZkyJh04ZI6HsKjfFdTe7MN8t5QryyFs7V2nTvC/pvP+jzkldhppFPG2MFoAK2N9jjL0fV2uut3H7XLI+LsQfDipfZAe/YMcLl9V3MgMcb+27WdsV+CQzkNphp2YgJs9GRJSYkpLS0dsddr7+pm8fffY0pOAs9+2YuTfxx+D1550O7Zzb3f/kdurnHCvMaGeVcfc+dEp0BCDsRnQfG1sPgvfP/HXOroenj2U3D3Gphw4/C/Xk831O2Fuo+dQN8PDQfg5GH7K6ZXfA6kT4C0a5zrCRCbAScPQd0+O4Ctbp/9Iuh9nLjsFMwZE+2XQLpzHZ9pw6atyeNy+uL7505fvu5MNeB89lPGQG6JE+olkDUVwiK9/7tbGmzg9wZ/wwG7PDLBHhQvWmqb0tKugbiMC18YgaTxqP2yazsN7Wfse9re7HHb47rdWWecGVIjE6HkS3aiwISAHRQf8kRkhzNu6fJ1oz3oAf7j3YP88O0DvPVXn2BCphcHSFsb4bdfh7LfQFgUxGdfCPGLbufYD35clm/3Rr3VeQ6+lw9J+XDNzfa4QMFC3+3dG2PD+cj79lK+3oYo2GBOLob0a2yQp19jgy5tPEQlePf83Z32S6Jur52Ns26v/QJoPHIhZK5I7GtFJTqXpAvXiXkX9tZjUgb5BvSjudYecO8N/8bDF9ZFJtr3IN15L9KcL7zkopFv9mlrgrJXYNeLlw+uc4Xb9y4y3n5hRSXa68h4Z3mCvY6Ig6N/gH1rbU+y6X9qmzszJo7s36I06AdyqqWDhd9/lxUzcvmXO6Z7/8COFttOG4h7aL12rIaP1thjA90dgNg91sLFFy6xqd4/35ka23R15Pc23Jur7fLEAhjzCftrJXOqbW+/mr3iq9HZZvea6/ZBa0MfQZ54IZhcAdDfoLnW+YVywF7qnWar3qYisMGaOvbi8E+fYH+1+PJ97Om2v0p3vQgf/w90tdlmsZl3wYSbICbVBnhY1NV9rhuP2DmjPnzO/qKdsBwW/YX9RRPI/z/60tMDp8vtv1lTlW2WuvRLrveL7mrfp+4u6Gi++BdTe7O9tDXZz+20OwZVtga9F771ym7WbK9k4yN/RHr8MAWUP3W2QVWpnXrh2EY7aKu3iSl9kp1Fs3CRPUgcn3nhcW1Ndu+0N9gb9tvl0ck21Mdcaw9WJxcH339of2trsoHvGf4NB5xfLN12G3cEZE2/0KyUN2dw73XdPtj5gv3SP1trvxSn3QEz7obc2b77t2s5CdufhG1P2Jlhc0tsM+XETwXe8QpjoKnCNjV6/mqsP9B382tfXGF9fwm4wy4EeLtHsHcOMJ9Wzix7PHAQNOi9cKT+LJ/8t9/z1T8az9f/eMKIv/6I6+qwByOPbbDhX7H1woHR1HGQN9cGT/UHtpkkLNp+EfQGe+a0wNhbDkVdHfbgdd1e50DxDqjZeaEHUEyqbXLKm2uvc+f03RzX0gC7X7Z77zU7bSiNXwYz7rLHbIbrFxfYbsQ7n4fN/2kn10sZAwsfhpl3Q3j08L1uX7raoaXefpmebwL82N727AwQn+0c+3E6AKRPguRC+763X7oX7nnMoo913Z0XvgAu/RI4fz/+4l8HvfcHeWBbg95LX3m2lA+On2LTI39EVHiA7X0Mt+4uqN3l7PFvsnv/ycUXgj1v7vAGg7qy7i4n+Ett8FeV2uDqPaCcOv7CMYfoZNjzazi4zk6FnTXdBuzUOyBu+CcNvEhPN+x7DTb+2O40xKTBvFUw7/6Bj40YY+vv7nAunTa028/Ynm2el9bGS5advnD70r3o2PTLAz3DmR8qiGnQe2nLkZPc+cQW/vkz07h7foFfalDKa21NF7qIVpba8G9xpvmOy4Tpn7N775lT/Fsn2NA+thE2/sR+AYVFQ9o4G969Ie4Z6L23veUKt18c0cnOpfd2kr2OSbFfhhmTfDfCPcBo0HvJGMMt/7mRlo4u3vmra3G5tM1ZBRFj4PRxe/A3d07gDt6q2wdb/x+cPWEH9rkjnEtft/tYFplwSagnB36niBFwpaAP0E+Cf4gIX1lazNde2sl7++v45KTMgR+kVKAQsW3KyYX+ruTKMibBp3/k7ypGFT2adombp2WTnRjFU+uP+rsUpZTyCQ36S4S7XXxpcRGbj5xkT1WTv8tRSqkh06Dvw5/OLSA2ws3TG3SvXikV/DTo+5AYHc6fzi3gtV3V1DR5OXBCKaUClAZ9P760uIgeY3h20zF/l6KUUkOiQd+P/JQYbpqazQtbj9HSPownCldKqWGmQX8FK5cWc6atizWlFf4uRSmlBk2D/gpmFyQzpzCZZzYepbsnsAaWKaWUtzToB/CVJcVUNJ7jrbLagTdWSqkApEE/gGVTsihIieEp7WqplApSAwa9iDwjInUisqef9SIiPxGRQyLykYjM9lh3n4gcdC73+bLwkeJ2CV9eXMSOY6f44Pgpf5ejlFJXzZs9+tXA8iusvwkY71xWAY8DiEgK8G1gPjAP+LaIBOU8oJ8tySchKoyndVoEpVQQGjDojTF/ABqvsMkK4BfG2gIkiUg2cCPwtjGm0RhzCnibK39hBKzYyDDunl/IG3tqqGhs9Xc5Sil1VXzRRp8LePY/rHSW9bf8MiKySkRKRaS0vr7eByX53n2LCnGJ8MxG3atXSgWXgDgYa4x5whhTYowpSU8f4TPgeCk7MZpPz8hhzfYKms51+rscpZTymi+CvgrI97if5yzrb3nQWrmkmJaObl7adtzfpSillNd8EfRrgS84vW8WAE3GmBpgHbBMRJKdg7DLnGVBa2puIgvHpLJ6Uzmd3T3+LkcppbziTffKF4HNwDUiUikiK0XkARF5wNnkdeAIcAh4EngQwBjTCPwjsN25POosC2r3f6KYmqY2Xt9d4+9SlFLKK3rO2KvU02O44d9/T0yEm9ceXoKM8vNUKqUCw5XOGRsQB2ODicslfGXJGPZUnWHr0aD/gaKUGgU06Afhttm5pMRG8NT6I/4uRSmlBqRBPwhR4W7uXVDIO/vqOFJ/1t/lKKXUFWnQD9LnFxQS4Xbx843l/i5FKaWuSIN+kNLjI1kxM4eXd1RyurXD3+UopVS/NOiHYOXSYs51dvOCDqBSSgUwDfohmJiVwJJxaTy7qZyOLh1ApZQKTBr0Q7RyaTEnzrTrACqlVMDSoB+ia8enMzY9lqc2HCHQBp8ppRRo0A+ZyyWsdAZQbdMBVEqpAKRB7wO3zc4lOSZczyurlApIGvQ+cGEA1QnKG1r8XY5SSl1Eg95HPr+gkDCX8HM9A5VSKsBo0PtIRkIUt8zI5Vc7KvUMVEqpgKJB70MrlxTTqmegUkoFGA16H5qck8CisXoGKqVUYNGg97GVS+wZqN7YU+vvUpRSCtCg97nrr8lgTFosT6/XAVRKqcCgQe9jLpfwpSXF7KpsYsexU36tpafH8BcvfsivSiv8WodSyr806IfB7bNzSYwO56n1/u1q+caeWtbuquan7x/WXxdKjWIa9MMgJiKMe+YX8NbeWo6fbPVLDV3dPfzwrf2Eu4WjDS3srmrySx1KKf/ToB8mX1hYhEuEn2/yz179yzsqOdLQwvdum06E28UrH1b7pQ6llP9p0A+TrMQoPj0jhzXbKzjTNrIDqNo6u/nxuweZVZDE7bNzue6adF77qJruHm2+UWo08iroRWS5iOwXkUMi8kgf6wtF5F0R+UhE3heRPI913SKy07ms9WXxgW7lkmJaOrr55baRPRj63JZj1DS18c0bJyIi3Dorl/rmdjYfPjmidSilAsOAQS8ibuAx4CZgMnCXiEy+ZLMfAL8wxkwHHgW+57HunDFmpnO5xUd1B4WpuYnML05h9aZyukZoAFVzWyePvXeIpePTWDg2FYA/mphBXGQYr+6sGpEalFKBxZs9+nnAIWPMEWNMB/ASsOKSbSYDv3Nuv9fH+lFr5ZJiqk6fY13ZiRF5vSfXH+VUayffvHHi+WVR4W6WT83izT21tHV2j0gdSqnA4U3Q5wKebQ+VzjJPu4DbnNufAeJFJNW5HyUipSKyRURu7esFRGSVs01pfX39VZQf+D45KZOi1Bie2nBk2F+r4Ww7T60/wp9My2ZaXuJF61bMzKG5vYv3Pq4b9jqUUoHFVwdjvwFcKyIfAtcCVUDvrmOhMaYEuBv4kYiMvfTBxpgnjDElxpiS9PR0H5UUGNwu4UuLi/nw+OlhH0D12HuHaO/q4evLJly2btHYNNLjI3lFm2+UGnW8CfoqIN/jfp6z7DxjTLUx5jZjzCzg75xlp53rKuf6CPA+MGvoZQeXO+bkkRAVxjPDeAaqylOtPL/lOJ+dk8fY9LjL1rtdwqen5/Dex/U0teo0ykqNJt4E/XZgvIgUi0gEcCdwUe8ZEUkTkd7n+hvgGWd5sohE9m4DLAb2+qr4YBEbGcZd8wt4Y08NFY3DM4DqR+8cBIGv3TC+321WzMyho7uHN8tqhqUGpVRgGjDojTFdwMPAOmAfsMYYUyYij4pIby+a64D9InIAyAS+6yyfBJSKyC7sQdrvG2NGXdADfHGRHUD17KZynz/3wRPN/PqDSr6woJDsxOh+t5uel0hxWqwOnlJqlAnzZiNjzOvA65cs+weP2y8DL/fxuE3AtCHWGBKyE6O5eVo2v9xewdduGE98VLjPnvuHbx0gJiKMB68fd8XtRIRbZuTwk98dpLapjazEKJ/VoJQKXDoydgR9ZWkxze1drCmt9Nlz7qw4zZtltdy/dAwpsREDbr9iZg7GwGu7dK9eqdFCg34ETc9LYm5RMj/7/WH21ZzxyXP+67qPSY2NYOXSYq+2H5Mex/S8RF7dpb1vlBotNOhH2KMrpuISuP3xTby5Z2gHRTccbGDjoZM8dP044iK9aoUDYMXMXPZUneFQ3dkhvb5SKjho0I+wSdkJvPbwEq7JiueB5z7g398+QM8gJhszxvCv6z4mNymaexYUXNVjPz09G5fAWu1Tr9SooEHvBxkJUbx4/wLumJPHj989yJ8/v4OW9q6reo51ZbXsqmziazeMJzLMfdWvv2hsGq/srNYTkig1CmjQ+0lUuJt/vWM6f/+pyby99wS3P77J6z72Xd09/Ou6/YzLiOO2WZfORuGdW2bmcLyxlQ8rTg/q8Uqp4KFB70ciwsolxaz+0jyqT5/jlv/cwKbDDQM+7tcfVnG4voVvLJtAmHtw/4TLp2YREeZi7U7tfaNUqNOgDwCfmJDOqw8vITUuks8/vY1fbC7vt0mlrbObH79zkBl5idw4JWvQr5kQFc4NkzL4n4+qR2wKZaWUf2jQB4jitFh+8+AirpuQzj+8Wsbf/mY3HV2XB/DzW49Tdfoc31xuTyoyFLfMyKXhbAcb9YQkSoU0DfoAEh8VzhNfKOHB68by4rYK7nlqCw1n28+vP9vexWPvHWLxuFQWj0sb8utdPzGd+KgwXv1Qe98oFco06AOM2yV8c/lEfnLXLHZXNXHLf2xgT1UTAE+tP0JjSwf/y+OkIkMRGebm5qnZrCur5VyHnpBEqVClQR+gbpmRw8sPLMIAd/xsE89tOcZT64+yfEoWM/OTfPY6K2bl0NLRzTv7RuYMWEqpkadBH8Cm5iay9uElTMlJ5Fuv7KG1o4tv3Hj5SUWGYn5xKpkJkbyqvW+UClnej5tXfpEeH8kL98/nB+v2kxQTwbiMeJ8+v9tlZ7Rcvamc060dJMUMPDGaUiq46B59EIgMc/N3fzKZhwaYhniwVszMpbPb8NvdekISpUKRBr1iSk4CY9NjtflGqRClQa8QEVbMzGXb0UaqTp/zdzlKKR/ToFeAPSEJ6AlJlApFGvQKgMLUWGbmJ/GKDp5SKuRo0Kvzbp2Zw8e1zeyvbfZ3KUopH9KgV+f9yfQc3C7hVT0hiVIhRYNenZceH8nicWm8qickUSqkeBX0IrJcRPaLyCEReaSP9YUi8q6IfCQi74tInse6+0TkoHO5z5fFK9+7dWYOVafPsePYKX+XopTykQGDXkTcwGPATcBk4C4RmXzJZj8AfmGMmQ48CnzPeWwK8G1gPjAP+LaIJPuufOVry6ZkERXu0j71SoUQb/bo5wGHjDFHjDEdwEvAiku2mQz8zrn9nsf6G4G3jTGNxphTwNvA8qGXrYZLXGQYN0zK5Le7a+jUE5IoFRK8CfpcoMLjfqWzzNMu4Dbn9meAeBFJ9fKxKsCsmJlLY0sHb+yp9XcpSikf8NXB2G8A14rIh8C1QBXg9QTnIrJKREpFpLS+vt5HJanBunZCOpOyE/hfv9rFpkMDn8NWKRXYvAn6KiDf436es+w8Y0y1MeY2Y8ws4O+cZae9eayz7RPGmBJjTEl6evpV/gnK1yLCXDy3ch5FqbF8+dntbNZTDSoV1LwJ+u3AeBEpFpEI4E5grecGIpImIr3P9TfAM87tdcAyEUl2DsIuc5apAJcaF8nz988nPzmGL6/ezpYjGvZKBasBg94Y0wU8jA3ofcAaY0yZiDwqIrc4m10H7BeRA0Am8F3nsY3AP2K/LLYDjzrLVBBIi4vkhfsXkJsczZd+vp2tGvZKBSUJtIExJSUlprS01N9lKA91zW3c9cQWapraePbL85hblOLvkpRSlxCRHcaYkr7W6chYNaCM+ChevH8BWYlRfPGZbZSW648ypYKJBr3ySkZCFC/dv4DMhCi++PPtOnJWqSCiQa+8lpEQxQv3LyAtLoL7ntnGB8c17JUKBhr06qpkJUbx4qoFpMZFcN/T29hZcdrfJSmlBqBBr65admI0L96/gOTYCD7/9FZ2adgrFdA06NWg5CRF87yODkoAAA8USURBVOKqBSTFhHPv01v5qFLDXqlApUGvBi03ye7ZJ0aHc+9TW9lT1eTvkpRSfdCgV0OSlxzDi/cvID4qnHtGKOxb2rt4fXcNf/nSh3z+6a2s3niU2qa2YX9dpYKVDphSPlHR2MqdT2zhbHsXt87MYW5xCvOKUshIiPLJ8zecbefdfSd4q+wE6w810NHVQ3JMOCmxERyubwFgdkESN0/LZvnULPKSY3zyukoFiysNmNKgVz5z/GQrf//qHrYdbeRcp528tCg1hrlFKcwtTmF+cQoFKTGIiNfP99beWt4qO0HpsUZ6jG0uunFKFsumZFJSmEyY28WhurO8uaeG13fXsrfmDADT8xJZPjWLm6ZmU5wWO2x/s1KBQoNejajO7h7Kqs+w7ehJth09RemxRk63dgKQER95fm9/XnEK12TG43LZ4DfGUFZ9hrf2nuCtslo+rm0GYFJ2AssmZ7JsSiaTsxOu+EVx7GQLb+yp5Y09ted7A03MiufmadncNDWL8Znxw/zXK+UfGvTKr3p6DIfqz7LtaCPbyxvZfrSRaqdNPSEqjJKiFLITo3h/fz1Vp8/hEigpSrHhPjmLgtTBNcNUnT7Hm3tqeWN3DTuOn8IYGJcRx01Ts/jsnPxBP69SgUiDXgWcylOt54N/29FGqk6fY8m4NJZNzuKTkzJIjYv06eudONPGurJa3thdy9ajJ4mNDGP1l+Yxp1BPYaxCgwa9Uh4qGlv5/NNbqWtu56kvlLBoXJq/S1JqyHT2SqU85KfEsObPFpKXHM0XV2/ndx+f8HdJSg0rDXo1KmUkRPHLVQu5JjOeVb/YwW8/qvF3SUoNGw16NWolx0bw/P3zmVWQxFdf/ICXd1T6uySlhoUGvRrVEqLCefbL81g8Lo1v/GoX/7W53N8lKeVzGvRq1IuJCOPJL5Rww6RM/v7VMn72+8P+Lkkpn9KgVwqICnfz+L2zuWVGDt9/42P+7a39BFqPNKUGK8zfBSgVKMLdLv79T2cSE+HmJ787REtHN9/6k0leT9mgVKDSoFfKg9slfO+2aURHuHl6w1FaO7r4p1un4XZp2KvgpUGv1CVEhH/41GTiIsP4j98dorWjmx98dgbhbm3pVMHJq0+uiCwXkf0ickhEHuljfYGIvCciH4rIRyJys7O8SETOichO5/IzX/8BSg0HEeGvl13D/14+kVd3VvPg8x/Q3tXt77KUGpQBg15E3MBjwE3AZOAuEZl8yWbfAtYYY2YBdwI/9Vh32Bgz07k84KO6lRoRf37dWP7PLVN4e+8JvvJsKa0dXf4uSamr5k3TzTzgkDHmCICIvASsAPZ6bGOABOd2IlDtyyKV8qf7FhURHeHmkf/+iPnffZfrJ2awfGoW105IJzZSWz9V4PPmU5oLVHjcrwTmX7LNd4C3ROSrQCxwg8e6YhH5EDgDfMsYs/7SFxCRVcAqgIKCAq+LV2qkfK4kn+K0WF4ureTtfSdYu6uaiDAXnxifxrIpWdwwKZOU2Ah/l6lUn3y1O3IXsNoY80MRWQj8l4hMBWqAAmPMSRGZA7wiIlOMMWc8H2yMeQJ4AuzslT6qSSmfmluUwtyiFL7b3UPpsVOsK7Nnv3pnXx1ulzCvKIUbp2SybEoWOUnR/i5XqfMGnKbYCe7vGGNudO7/DYAx5nse25QBy40xFc79I8ACY0zdJc/1PvANY0y/8xDrNMUqmPSeFevNPbWsK6vlYN1ZwJ7K8MYpWdw4JZNxGXpWKzX8hjQfvYiEAQeATwJVwHbgbmNMmcc2bwC/NMasFpFJwLvYJp80oNEY0y0iY4D1wDRjTGN/r6dBr4LZkfqzrCs7wZtlF05lOCY9loVjUpmRn8TM/CTGpsdpv3zlc0M+8YjTXfJHgBt4xhjzXRF5FCg1xqx1euE8CcRhD8x+0xjzlojcDjwKdAI9wLeNMa9d6bU06FWoqGk6x9t7T/D23hPsPH6a5nbbYyc2ws20vEQb/HlJzMhPIjsxSkfgqiHRM0wp5Wc9PYYjDS3sqjjNrsrT7Ko4zd6aM3R22/9/6fGRzMhLYma+/QKYnptEYky4n6tWweRKQa99w5QaAS6XMC4jjnEZcdw+Jw+A9q5u9tU02/B3vgDe2XfhbFdj0mKZU5jM3KIUSoqSKU6L1b1+NSga9Er5SWSYm5lOu32vpnOd7KlqYmfFaT48boP/V84JUdLiIigptKE/rziFydkJhOm0DMoLGvRKBZDE6HAWj0tjsXPCctvkc5ZtR09RWt7I9mONvFlWC0BMhJtZBUnnu33OzE/SAVyqT9pGr1SQqW1qY3t5I6XljWwrP8XHtWcwxs68OSUngTmFyUzLTWRKTiJj02N1r3+U0IOxSoWwM22dfHDsFKXlp9hW3siuitO0d/UAEBnmYmJ2AlNyei+JTMyKJyrc7eeqla9p0Cs1inR193CkoYWy6ibKqs6wp7qJsuozNLfZ7p1ulzAuPY4pOQlMzklgam4ik3MSSIjSXj7BTINeqVHOGEPlqXM2/KvPUFZ9hj1VTdQ1t5/fJj0+ktgIN9ERYcREuImJcBMd7lx7LotwExPuJiYijOgIN5kJUUzKjidevyj8SrtXKjXKiQj5KTHkp8SwfGr2+eX1ze3nw//4yVZaO7s519FFa0c3zW1d1J1pp7Wzi3MdPXZ5Zzf97RsWpsYw2Wkmmuw0E2XER2qX0ACgQa/UKJYeH8l112Rw3TUZXm1vjKG9q4fWjm5aO7o419F9/pfC3hr7S+GNPbXnt0+NjWCyR/BPyUmgKDVWp4AYYRr0SimviQhR4W6iwt3np2UenxnP9RMvfFGcaevk45pmG/5OM9EzG46eHwUcHe5mUnY8cwqTWTI+nXlFKURH6MHh4aRt9EqpYdfR1cPBuubzwV9W3cSuiiY6unuIcLsoKUpmyfg0lo5LZ0pOAi7d479qejBWKRVwWju62Ha0kQ0HG9hwqIGPa5sBSI4JZ9G4NJaOS2PJ+DTykmP8XGlw0IOxSqmAExMRdtHxgbrmNjYeamD9wQY2HGzgtx/VAFCcFssSJ/QXjk3VbqCDoHv0SqmAY4zhYN1ZJ/Tr2Xq0kdaOblxig39cRhzjM+IZn2knihubHjfqB4HpHr1SKqiICBMy45mQGc/KJcV0dPXw4fFTbDp8kv21zRysa+adfXV09xhneyhIiWF8RhzjMuKdLwL7JaDz/2jQK6WCQESYi/ljUpk/JvX8so6uHspPtnDwxFkO1jVzsO4sh+vO8ocDDXR095zfLjcpmjHpsSTHRBAfFUZcVBgJUeH2dmQY8VHhzrVdHhdlb4eH0BxBGvRKqaAUEeY6v9cPFwaBdXX3cLyxlYN1ZzlUd5aDJ5o50tDC8cZWzrZ10dzWddEXQX8iw1znfw0YYzCAMRduY3CWeazDYAxER7gpTotlbHocY9Lt9dj0WApSYokIG/kvEG2jV0qNOu1dduRvb/A3t3XS3N7lLOu01+1dtHTY+YEEQQQEzo/0tfc9l9t1AjS3d3G0voXD9WcvmmbC7RIKUmIYkxbL2Iy4i65TYiOGNIpY2+iVUspDZJibyDg3aXGRw/5azW2dHKlv4UjDWQ7XXbhef6iBjq4LvyySYsJZOj6d/7hrls9r0KBXSqlhFB8Vzox8exJ4T909hqpT5zjcYI8tHGloISl6eLqOatArpZQfuF1CQWoMBakxXO/lXEODFTqHlZVSSvVJg14ppUKcV0EvIstFZL+IHBKRR/pYXyAi74nIhyLykYjc7LHub5zH7ReRG31ZvFJKqYEN2EYvIm7gMeCPgUpgu4isNcbs9djsW8AaY8zjIjIZeB0ocm7fCUwBcoB3RGSCMabb13+IUkqpvnmzRz8POGSMOWKM6QBeAlZcso0BEpzbiUC1c3sF8JIxpt0YcxQ45DyfUkqpEeJN0OcCFR73K51lnr4D3Csildi9+a9exWMRkVUiUioipfX19V6WrpRSyhu+Ohh7F7DaGJMH3Az8l4h4/dzGmCeMMSXGmJL09HQflaSUUgq860dfBeR73M9zlnlaCSwHMMZsFpEoIM3LxyqllBpGA851IyJhwAHgk9iQ3g7cbYwp89jmDeCXxpjVIjIJeBfbRDMZeAHbLp/jLB9/pYOxIlIPHBvC35QGNAzh8aFI35PL6XtyOX1PLhdM70mhMabPJpEB9+iNMV0i8jCwDnADzxhjykTkUaDUGLMW+GvgSRH5K+yB2S8a+w1SJiJrgL1AF/DQQD1u+ivUWyJS2t/EPqOVvieX0/fkcvqeXC5U3pOAm71yqELlH8aX9D25nL4nl9P35HKh8p7oyFillApxoRj0T/i7gACk78nl9D25nL4nlwuJ9yTkmm6UUkpdLBT36JVSSnnQoFdKqRAXMkE/0Aybo5GIlIvIbhHZKSKj9kS8IvKMiNSJyB6PZSki8raIHHSuk/1Z40jr5z35johUOZ+XnZ6z0I4GIpLvzMK7V0TKRORrzvKg/6yERNB7zLB5E3aQ1l3OzJkKrjfGzAyFLmJDsBpn5LaHR4B3jTHjsQP5RtvOwWouf08A/t35vMw0xrw+wjX5Wxfw18aYycAC4CEnR4L+sxISQY93M2yqUcoY8weg8ZLFK4BnndvPAreOaFF+1s97MqoZY2qMMR84t5uBfdgR/kH/WQmVoPdqlsxRyABvicgOEVnl72ICTKYxpsa5XQtk+rOYAPKwc/KgZ4KxicJXRKQImAVsJQQ+K6ES9KpvS4wxs7FNWg+JyCf8XVAgcqbr0H7G8DgwFpgJ1AA/9G85/iEiccB/A39pjDnjuS5YPyuhEvQ6S2YfjDFVznUd8Bv0pC+eTohINoBzXefnevzOGHPCGNNtjOkBnmQUfl5EJBwb8s8bY37tLA76z0qoBP12YLyIFItIBPb0hWv9XJNfiUisiMT33gaWAXuu/KhRZS1wn3P7PuBVP9YSEHrDzPEZRtnnRUQEeBrYZ4z5N49VQf9ZCZmRsU5XsB9xYYbN7/q5JL8SkTHYvXiws5S+MFrfExF5EbgOO+XsCeDbwCvAGqAAOy3254wxo+bgZD/vyXXYZhsDlAN/5tE2HfJEZAmwHtgN9DiL/xbbTh/Un5WQCXqllFJ9C5WmG6WUUv3QoFdKqRCnQa+UUiFOg14ppUKcBr1SSoU4DXqllApxGvRKKRXi/j8rIywFC2mL+QAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load('tut1-model.pt'))\n",
        "true, predicted, acc, test_loss = evaluate(model, test_loader, criterion)"
      ],
      "metadata": {
        "id": "SSGOvSpTlsT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"{100*acc:.1f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLLuUcWylv13",
        "outputId": "12d78284-7e22-415f-c2c1-5e98b82b591e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "51.8%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_tr = [int(torch.max(i).item()) for i in true]\n",
        "y_pr = [int(torch.max(i).item()) for i in predicted]"
      ],
      "metadata": {
        "id": "FyJHBSe_6IQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "true_ = []\n",
        "for i in true:\n",
        "  i.tolist()\n",
        "  true_.extend(i.tolist())\n",
        "\n",
        "predicted_ = []\n",
        "for i in predicted:\n",
        "  predicted_.extend(torch.argmax(i,1).tolist())"
      ],
      "metadata": {
        "id": "4MLeirKE0zQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(predicted_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7wUa_h_78TR",
        "outputId": "5f4e991b-b0e4-43d1-9720-096152bb88bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5067"
            ]
          },
          "metadata": {},
          "execution_count": 171
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "print(classification_report(true_, predicted_))\n",
        "print(confusion_matrix(true_, predicted_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wpiu2yX20Bkm",
        "outputId": "e956a4bd-5f46-4148-d6a1-962b39032b80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.38      0.42      0.40      1298\n",
            "           1       0.61      0.61      0.61      2163\n",
            "           2       0.53      0.47      0.50      1606\n",
            "\n",
            "    accuracy                           0.52      5067\n",
            "   macro avg       0.50      0.50      0.50      5067\n",
            "weighted avg       0.52      0.52      0.52      5067\n",
            "\n",
            "[[ 545  435  318]\n",
            " [ 474 1321  368]\n",
            " [ 426  420  760]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model, \"/content/drive/MyDrive/Colab_Notebooks/Deep_Learning/deepproject/text_model.pkl\")"
      ],
      "metadata": {
        "id": "ah2Scn-5kIGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Domain Adaptation via Weakly Supervised Learning"
      ],
      "metadata": {
        "id": "YpwCyaVQwQFu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "COCO Dataset"
      ],
      "metadata": {
        "id": "fm5Ng4W2g2pt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#           annotations_trainval2014.zip\n",
        "\n",
        "!wget http://images.cocodataset.org/annotations/annotations_trainval2014.zip\n",
        "!unzip /content/annotations_trainval2014.zip\n",
        "!rm annotations_trainval2014.zip"
      ],
      "metadata": {
        "id": "ybmwGuj2g5Js",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5330bb2-4cb4-41b2-9441-c69e76af44ef"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-12 08:24:14--  http://images.cocodataset.org/annotations/annotations_trainval2014.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 52.216.33.33, 52.216.245.116, 54.231.131.201, ...\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|52.216.33.33|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 252872794 (241M) [application/zip]\n",
            "Saving to: ‘annotations_trainval2014.zip’\n",
            "\n",
            "annotations_trainva 100%[===================>] 241.16M  33.5MB/s    in 7.9s    \n",
            "\n",
            "2023-02-12 08:24:22 (30.4 MB/s) - ‘annotations_trainval2014.zip’ saved [252872794/252872794]\n",
            "\n",
            "Archive:  /content/annotations_trainval2014.zip\n",
            "  inflating: annotations/instances_train2014.json  \n",
            "  inflating: annotations/instances_val2014.json  \n",
            "  inflating: annotations/person_keypoints_train2014.json  \n",
            "  inflating: annotations/person_keypoints_val2014.json  \n",
            "  inflating: annotations/captions_train2014.json  \n",
            "  inflating: annotations/captions_val2014.json  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#                 train2014.zip\n",
        "\n",
        "!wget http://images.cocodataset.org/zips/train2014.zip\n",
        "!unzip -q /content/train2014.zip\n",
        "!rm train2014.zip"
      ],
      "metadata": {
        "id": "RiFf_t5BhQn9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b2995b2-39db-4a32-b555-f34c7b78a5c2"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-12 08:24:31--  http://images.cocodataset.org/zips/train2014.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 52.217.93.20, 52.216.133.219, 52.217.68.76, ...\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|52.217.93.20|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13510573713 (13G) [application/zip]\n",
            "Saving to: ‘train2014.zip’\n",
            "\n",
            "train2014.zip       100%[===================>]  12.58G  33.8MB/s    in 6m 37s  \n",
            "\n",
            "2023-02-12 08:31:08 (32.5 MB/s) - ‘train2014.zip’ saved [13510573713/13510573713]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#                 COCO dataset\n",
        "\n",
        "!git clone https://github.com/cocodataset/cocoapi.git\n",
        "!coco/PythonAPI\n",
        "!make"
      ],
      "metadata": {
        "id": "1YwfIRgshX_o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1eae5033-ec5a-40cc-85cf-c8cbd83d6d4c"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'cocoapi'...\n",
            "remote: Enumerating objects: 975, done.\u001b[K\n",
            "remote: Total 975 (delta 0), reused 0 (delta 0), pack-reused 975\u001b[K\n",
            "Receiving objects: 100% (975/975), 11.72 MiB | 22.65 MiB/s, done.\n",
            "Resolving deltas: 100% (576/576), done.\n",
            "/bin/bash: coco/PythonAPI: No such file or directory\n",
            "make: *** No targets specified and no makefile found.  Stop.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Vocabulary(object):\n",
        "\n",
        "    def __init__(self, vocab_threshold, vocab_file='/content/drive/MyDrive/Colab_Notebooks/Deep_Learning/hw4/vocab.pkl',\n",
        "        start_word=\"<sos>\", end_word=\"<eos>\", unk_word=\"<unk>\",\n",
        "        annotations_file='/content/annotations/captions_train2014.json', vocab_from_file=False):\n",
        "        self.vocab_threshold = vocab_threshold\n",
        "        self.vocab_file = vocab_file\n",
        "        self.start_word = start_word\n",
        "        self.end_word = end_word\n",
        "        self.unk_word = unk_word\n",
        "        self.annotations_file = annotations_file\n",
        "        self.vocab_from_file = vocab_from_file\n",
        "        self.get_vocab()\n",
        "\n",
        "    def get_vocab(self):\n",
        "        \"\"\"Load the vocabulary from file OR build the vocabulary from scratch.\"\"\"\n",
        "        if os.path.exists(self.vocab_file) & self.vocab_from_file:\n",
        "            with open(self.vocab_file, 'rb') as f:\n",
        "                vocab = pickle.load(f)\n",
        "                self.word2idx = vocab.word2idx\n",
        "                self.idx2word = vocab.idx2word\n",
        "            print('Vocabulary successfully loaded from vocab.pkl file!')\n",
        "        else:\n",
        "            self.build_vocab()\n",
        "            with open(self.vocab_file, 'wb') as f:\n",
        "                pickle.dump(self, f)\n",
        "\n",
        "    def build_vocab(self):\n",
        "        \"\"\"Populate the dictionaries for converting tokens to integers (and vice-versa).\"\"\"\n",
        "        self.init_vocab()\n",
        "        self.add_word(self.start_word)\n",
        "        self.add_word(self.end_word)\n",
        "        self.add_word(self.unk_word)\n",
        "        self.add_captions()\n",
        "\n",
        "    def init_vocab(self):\n",
        "        \"\"\"Initialize the dictionaries for converting tokens to integers (and vice-versa).\"\"\"\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = {}\n",
        "        self.idx = 0\n",
        "\n",
        "    def add_word(self, word):\n",
        "        \"\"\"Add a token to the vocabulary.\"\"\"\n",
        "        if not word in self.word2idx:\n",
        "            self.word2idx[word] = self.idx\n",
        "            self.idx2word[self.idx] = word\n",
        "            self.idx += 1\n",
        "\n",
        "    def add_captions(self):\n",
        "        \"\"\"Loop over training captions and add all tokens to the vocabulary that meet or exceed the threshold.\"\"\"\n",
        "        coco = COCO(self.annotations_file)\n",
        "        counter = Counter()\n",
        "        ids = coco.anns.keys()\n",
        "        for i, id in enumerate(ids):\n",
        "            caption = str(coco.anns[id]['caption'])\n",
        "            # Remove punctuation by using regex\n",
        "            rp_caption = re.sub(r'[^\\w\\s]','',caption)\n",
        "            #remove numbers\n",
        "            rnp_caption = re.sub(r'\\d+', '', rp_caption)\n",
        "            tokens = nltk.tokenize.word_tokenize(rnp_caption.lower())\n",
        "            counter.update(tokens)\n",
        "\n",
        "            if i % 100000 == 0:\n",
        "                print(\"[%d/%d] Tokenizing captions...\" % (i, len(ids)))\n",
        "\n",
        "        words = [word for word, cnt in counter.items() if cnt >= self.vocab_threshold]\n",
        "\n",
        "        for i, word in enumerate(words):\n",
        "            self.add_word(word)\n",
        "\n",
        "    def __call__(self, word):\n",
        "        if not word in self.word2idx:\n",
        "            return self.word2idx[self.unk_word]\n",
        "        return self.word2idx[word]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.word2idx)"
      ],
      "metadata": {
        "id": "1QRZG6k7rwxE"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CoCoDataset(data.Dataset):\n",
        "\n",
        "    def __init__(self, transform, mode, batch_size, vocab_threshold, vocab_file, start_word, \n",
        "        end_word, unk_word, annotations_file, vocab_from_file, img_folder):\n",
        "        self.transform = transform\n",
        "        self.mode = mode\n",
        "        self.batch_size = batch_size\n",
        "        self.vocab = Vocabulary(vocab_threshold, vocab_file, start_word,\n",
        "            end_word, unk_word, annotations_file, vocab_from_file)\n",
        "        self.img_folder = img_folder\n",
        "        if self.mode == 'train':\n",
        "            self.coco = COCO(annotations_file)\n",
        "            self.ids = list(self.coco.anns.keys())\n",
        "            print('Obtaining caption lengths...')\n",
        "            all_tokens = [nltk.tokenize.word_tokenize(str(self.coco.anns[self.ids[index]]['caption']).lower()) for index in tqdm(np.arange(len(self.ids)))]\n",
        "            self.caption_lengths = [len(token) for token in all_tokens]\n",
        "        else:\n",
        "            test_info = json.loads(open(annotations_file).read())\n",
        "            self.paths = [item['file_name'] for item in test_info['images']]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # obtain image and caption if in training mode\n",
        "        if self.mode == 'train':\n",
        "            ann_id = self.ids[index]\n",
        "            caption = self.coco.anns[ann_id]['caption']\n",
        "            img_id = self.coco.anns[ann_id]['image_id']\n",
        "            path = self.coco.loadImgs(img_id)[0]['file_name']\n",
        "\n",
        "            # Convert image to tensor and pre-process using transform\n",
        "            image = Image.open(os.path.join(self.img_folder, path)).convert('RGB')\n",
        "            image = self.transform(image)\n",
        "\n",
        "            # Convert caption to tensor of word ids.\n",
        "            tokens = nltk.tokenize.word_tokenize(str(caption).lower())\n",
        "            caption = []\n",
        "            caption.append(self.vocab(self.vocab.start_word))\n",
        "            caption.extend([self.vocab(token) for token in tokens])\n",
        "            caption.append(self.vocab(self.vocab.end_word))\n",
        "            caption = torch.Tensor(caption).long()\n",
        "\n",
        "            # return pre-processed image and caption tensors\n",
        "            return image, caption\n",
        "\n",
        "        # obtain image if in test mode\n",
        "        else:\n",
        "            path = self.paths[index]\n",
        "\n",
        "            # Convert image to tensor and pre-process using transform\n",
        "            PIL_image = Image.open(os.path.join(self.img_folder, path)).convert('RGB')\n",
        "            orig_image = np.array(PIL_image)\n",
        "            image = self.transform(PIL_image)\n",
        "\n",
        "            # return original image and pre-processed image tensor\n",
        "            return orig_image, image\n",
        "\n",
        "    def get_train_indices(self):\n",
        "        sel_length = np.random.choice(self.caption_lengths)\n",
        "        all_indices = np.where([self.caption_lengths[i] == sel_length for i in np.arange(len(self.caption_lengths))])[0]\n",
        "        indices = list(np.random.choice(all_indices, size=self.batch_size))\n",
        "        return indices\n",
        "\n",
        "    def __len__(self):\n",
        "        if self.mode == 'train':\n",
        "            return len(self.ids)\n",
        "        else:\n",
        "            return len(self.paths)"
      ],
      "metadata": {
        "id": "PZQ62pNprs3m"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---------------------------------------------------------"
      ],
      "metadata": {
        "id": "2b6Wzz8P4E5R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Text Network\n",
        "class BiDGRU(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, dropout):\n",
        "        super().__init__()\n",
        "        self.hid_dim = hid_dim\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.rnn = nn.GRU(emb_dim, hid_dim, bidirectional= True)\n",
        "        self.fc1 = nn.Linear(hid_dim * 2, hid_dim)\n",
        "        self.fc2 = nn.Linear(hid_dim, 3)\n",
        "        self.probs = nn.Softmax(dim=1)\n",
        "    def forward(self, src):\n",
        "        s = self.embedding(src)\n",
        "        embedded = self.dropout(s)\n",
        "        outputs, hidden = self.rnn(embedded)\n",
        "        hidden = torch.tanh(self.fc1(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
        "        hidden = self.fc2(hidden)\n",
        "        probabilities = self.probs(hidden)\n",
        "        return probabilities"
      ],
      "metadata": {
        "id": "H0Q3w-L2hcLo"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Image Network\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.features = vgg16_pretrained.features\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(output_size =(2,2))\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(in_features=2048, out_features=128, bias=True),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=0.5, inplace=False),\n",
        "            nn.Linear(in_features=128, out_features=16, bias=True),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=0.5, inplace=False),\n",
        "            nn.Linear(in_features=16, out_features=3, bias=True))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        y = self.classifier(x)\n",
        "        return y"
      ],
      "metadata": {
        "id": "-SlMCo7w1Luz"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_model = torch.load(\"/content/drive/MyDrive/Colab_Notebooks/Deep_Learning/project/text_model.pkl\")\n",
        "image_model = torch.load(\"/content/drive/MyDrive/Colab_Notebooks/Deep_Learning/project/Parvizi_Part2\", map_location=torch.device('cuda'))"
      ],
      "metadata": {
        "id": "xYyg0FUgj9jz"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for param in image_model.features.parameters():\n",
        "  param.requires_grad = False\n",
        "for name, param in image_model.named_parameters():\n",
        "  print(f\"Layer: {name} | Size: {param.size()} | grad : {param.requires_grad} \\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXMK0cnlQ4Ow",
        "outputId": "066d42eb-a019-44e6-8f39-586ff74b5d74"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer: features.0.weight | Size: torch.Size([64, 3, 3, 3]) | grad : False \n",
            "\n",
            "Layer: features.0.bias | Size: torch.Size([64]) | grad : False \n",
            "\n",
            "Layer: features.2.weight | Size: torch.Size([64, 64, 3, 3]) | grad : False \n",
            "\n",
            "Layer: features.2.bias | Size: torch.Size([64]) | grad : False \n",
            "\n",
            "Layer: features.5.weight | Size: torch.Size([128, 64, 3, 3]) | grad : False \n",
            "\n",
            "Layer: features.5.bias | Size: torch.Size([128]) | grad : False \n",
            "\n",
            "Layer: features.7.weight | Size: torch.Size([128, 128, 3, 3]) | grad : False \n",
            "\n",
            "Layer: features.7.bias | Size: torch.Size([128]) | grad : False \n",
            "\n",
            "Layer: features.10.weight | Size: torch.Size([256, 128, 3, 3]) | grad : False \n",
            "\n",
            "Layer: features.10.bias | Size: torch.Size([256]) | grad : False \n",
            "\n",
            "Layer: features.12.weight | Size: torch.Size([256, 256, 3, 3]) | grad : False \n",
            "\n",
            "Layer: features.12.bias | Size: torch.Size([256]) | grad : False \n",
            "\n",
            "Layer: features.14.weight | Size: torch.Size([256, 256, 3, 3]) | grad : False \n",
            "\n",
            "Layer: features.14.bias | Size: torch.Size([256]) | grad : False \n",
            "\n",
            "Layer: features.17.weight | Size: torch.Size([512, 256, 3, 3]) | grad : False \n",
            "\n",
            "Layer: features.17.bias | Size: torch.Size([512]) | grad : False \n",
            "\n",
            "Layer: features.19.weight | Size: torch.Size([512, 512, 3, 3]) | grad : False \n",
            "\n",
            "Layer: features.19.bias | Size: torch.Size([512]) | grad : False \n",
            "\n",
            "Layer: features.21.weight | Size: torch.Size([512, 512, 3, 3]) | grad : False \n",
            "\n",
            "Layer: features.21.bias | Size: torch.Size([512]) | grad : False \n",
            "\n",
            "Layer: features.24.weight | Size: torch.Size([512, 512, 3, 3]) | grad : False \n",
            "\n",
            "Layer: features.24.bias | Size: torch.Size([512]) | grad : False \n",
            "\n",
            "Layer: features.26.weight | Size: torch.Size([512, 512, 3, 3]) | grad : False \n",
            "\n",
            "Layer: features.26.bias | Size: torch.Size([512]) | grad : False \n",
            "\n",
            "Layer: features.28.weight | Size: torch.Size([512, 512, 3, 3]) | grad : False \n",
            "\n",
            "Layer: features.28.bias | Size: torch.Size([512]) | grad : False \n",
            "\n",
            "Layer: classifier.1.weight | Size: torch.Size([128, 2048]) | grad : True \n",
            "\n",
            "Layer: classifier.1.bias | Size: torch.Size([128]) | grad : True \n",
            "\n",
            "Layer: classifier.4.weight | Size: torch.Size([16, 128]) | grad : True \n",
            "\n",
            "Layer: classifier.4.bias | Size: torch.Size([16]) | grad : True \n",
            "\n",
            "Layer: classifier.7.weight | Size: torch.Size([3, 16]) | grad : True \n",
            "\n",
            "Layer: classifier.7.bias | Size: torch.Size([3]) | grad : True \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize(72),\n",
        "    transforms.RandomCrop(64),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])"
      ],
      "metadata": {
        "id": "GQhESPE7s_S7"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vocab_file = '/content/drive/MyDrive/Colab_Notebooks/Deep_Learning/hw4/vocab.pkl'\n",
        "# annotations_file = '/content/annotations/captions_train2014.json'\n",
        "# img_folder = '/content/images/test2014/'\n",
        "num_workers = 0\n",
        "\n",
        "train_dataset = CoCoDataset(transform=transform_train,\n",
        "                          mode='train',\n",
        "                          batch_size=64,\n",
        "                          vocab_threshold=6,\n",
        "                          vocab_file='/content/drive/MyDrive/Colab_Notebooks/Deep_Learning/project/vocab.pkl',\n",
        "                          start_word='<sos>',\n",
        "                          end_word='<eos>',\n",
        "                          unk_word='<ukn>',\n",
        "                          annotations_file=\"/content/annotations/captions_train2014.json\",\n",
        "                          vocab_from_file=False,\n",
        "                          img_folder='/content/train2014/')\n",
        "print(\"Vocabulary is creaated!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLaQqciQqp1Q",
        "outputId": "d63bb5f7-b1b9-4f98-d407-442950744eed"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=1.05s)\n",
            "creating index...\n",
            "index created!\n",
            "[0/414113] Tokenizing captions...\n",
            "[100000/414113] Tokenizing captions...\n",
            "[200000/414113] Tokenizing captions...\n",
            "[300000/414113] Tokenizing captions...\n",
            "[400000/414113] Tokenizing captions...\n",
            "loading annotations into memory...\n",
            "Done (t=1.09s)\n",
            "creating index...\n",
            "index created!\n",
            "Obtaining caption lengths...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 414113/414113 [00:49<00:00, 8400.66it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary is creaated!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = CoCoDataset(transform=transform_train,\n",
        "                          mode='train', batch_size=64, vocab_threshold=6,\n",
        "                          vocab_file='/content/drive/MyDrive/Colab_Notebooks/Deep_Learning/project/vocab.pkl',\n",
        "                          start_word='<sos>', end_word='<eos>', unk_word='<ukn>',\n",
        "                          annotations_file=\"/content/annotations/captions_train2014.json\",\n",
        "                          vocab_from_file=True, img_folder='/content/train2014/')\n",
        "indices = train_dataset.get_train_indices()\n",
        "initial_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
        "unlabeled_data_loader = data.DataLoader(dataset=train_dataset,\n",
        "                              num_workers=0,\n",
        "                              batch_sampler=data.sampler.BatchSampler(sampler=initial_sampler,batch_size=train_dataset.batch_size,drop_last=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kE88ud-ur3gU",
        "outputId": "91ca416d-8a49-4079-ceb4-4689f9eb5359"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary successfully loaded from vocab.pkl file!\n",
            "loading annotations into memory...\n",
            "Done (t=0.64s)\n",
            "creating index...\n",
            "index created!\n",
            "Obtaining caption lengths...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 414113/414113 [00:46<00:00, 8822.33it/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test(device, model, dataloader, loss_fn):\n",
        "    model.eval()\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "    predicted, true = [], []\n",
        "    with torch.no_grad():\n",
        "        for (X, txt , sntmnt, dilog) in dataloader:\n",
        "          X = X.to(device)\n",
        "          y = []\n",
        "          for lst in sntmnt:\n",
        "            elements = re.findall(r'\\d+', lst)\n",
        "            res = [int(x) for x in elements]\n",
        "            y.append(res[0])\n",
        "          y = torch.tensor(y).to(device)\n",
        "          pred = model(X)\n",
        "          true.append(y)\n",
        "          predicted.append(pred)\n",
        "          test_loss += loss_fn(pred, y).item()\n",
        "          correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"test Error:  Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "    return predicted, true"
      ],
      "metadata": {
        "id": "ak7DHVbK_Dhv"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted, true = test(device, best_model, test_dataloader, loss_fn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BiNIgYXU9_IW",
        "outputId": "d47f3610-16b7-4a67-c8b2-717ea07d701f"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test Error:  Accuracy: 41.1%, Avg loss: 1.093797 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_model = torch.load(\"/content/drive/MyDrive/Colab_Notebooks/Deep_Learning/project/Parvizi_Part2\", map_location=torch.device('cuda'))"
      ],
      "metadata": {
        "id": "L3J7NqXySIzk"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
        "params =list(image_model.classifier.parameters())\n",
        "optimizer = torch.optim.Adam(params, lr=0.0001, betas=(0.9, 0.999), eps=1e-08)"
      ],
      "metadata": {
        "id": "YLI-cClgPtxh"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 1\n",
        "print_every = 200\n",
        "vocab_size = len(unlabeled_data_loader.dataset.vocab)\n",
        "total_step = math.ceil(len(unlabeled_data_loader.dataset.caption_lengths) / unlabeled_data_loader.batch_sampler.batch_size)\n",
        "for epoch in range(1, num_epochs+1):\n",
        "    \n",
        "    for i_step in range(1, total_step+1):\n",
        "\n",
        "        indices = unlabeled_data_loader.dataset.get_train_indices()\n",
        "        new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
        "        unlabeled_data_loader.batch_sampler.sampler = new_sampler\n",
        "        images, captions = next(iter(unlabeled_data_loader))\n",
        "        images = images.to(device)\n",
        "        captions = captions.to(device)\n",
        "        image_model.zero_grad()\n",
        "        # outputs = image_model(images, captions)\n",
        "        outputs = image_model(images)\n",
        "        target = text_model(captions.T)\n",
        "        # values, indices = torch.max(target, axis=1)\n",
        "        loss = criterion(outputs, target.detach())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        stats = 'Epoch [%d/%d], Step [%d/%d], Loss: %.4f, Perplexity: %5.4f' % (epoch, num_epochs, i_step, total_step, loss.item(), np.exp(loss.item()))\n",
        "        print('\\r' + stats, end=\"\")\n",
        "        sys.stdout.flush()\n",
        "        if i_step % print_every == 0:\n",
        "            print('\\r' + stats)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FTu6mmM19NZu",
        "outputId": "b656a8eb-29e6-4291-aff2-dd15766a35bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/1], Step [200/6471], Loss: 0.9699, Perplexity: 2.6377\n",
            "Epoch [1/1], Step [400/6471], Loss: 1.0545, Perplexity: 2.8707\n",
            "Epoch [1/1], Step [600/6471], Loss: 1.0073, Perplexity: 2.7381\n",
            "Epoch [1/1], Step [800/6471], Loss: 1.0268, Perplexity: 2.7921\n",
            "Epoch [1/1], Step [1000/6471], Loss: 1.0433, Perplexity: 2.8385\n",
            "Epoch [1/1], Step [1200/6471], Loss: 1.0352, Perplexity: 2.8158\n",
            "Epoch [1/1], Step [1400/6471], Loss: 1.0430, Perplexity: 2.8378\n",
            "Epoch [1/1], Step [1600/6471], Loss: 0.9722, Perplexity: 2.6438\n",
            "Epoch [1/1], Step [1800/6471], Loss: 1.0781, Perplexity: 2.9391\n",
            "Epoch [1/1], Step [2000/6471], Loss: 1.0085, Perplexity: 2.7416\n",
            "Epoch [1/1], Step [2200/6471], Loss: 0.9664, Perplexity: 2.6285\n",
            "Epoch [1/1], Step [2400/6471], Loss: 0.9697, Perplexity: 2.6373\n",
            "Epoch [1/1], Step [2600/6471], Loss: 1.0735, Perplexity: 2.9255\n",
            "Epoch [1/1], Step [2800/6471], Loss: 0.9598, Perplexity: 2.6111\n",
            "Epoch [1/1], Step [3000/6471], Loss: 1.0649, Perplexity: 2.9006\n",
            "Epoch [1/1], Step [3200/6471], Loss: 1.0614, Perplexity: 2.8905\n",
            "Epoch [1/1], Step [3400/6471], Loss: 1.0262, Perplexity: 2.7904\n",
            "Epoch [1/1], Step [3600/6471], Loss: 0.9415, Perplexity: 2.5638\n",
            "Epoch [1/1], Step [3800/6471], Loss: 0.9322, Perplexity: 2.5402\n",
            "Epoch [1/1], Step [4000/6471], Loss: 1.0541, Perplexity: 2.8693\n",
            "Epoch [1/1], Step [4200/6471], Loss: 0.9509, Perplexity: 2.5880\n",
            "Epoch [1/1], Step [4400/6471], Loss: 1.0914, Perplexity: 2.9784\n",
            "Epoch [1/1], Step [4600/6471], Loss: 1.0148, Perplexity: 2.7589\n",
            "Epoch [1/1], Step [4800/6471], Loss: 1.0260, Perplexity: 2.7899\n",
            "Epoch [1/1], Step [5000/6471], Loss: 1.0887, Perplexity: 2.9704\n",
            "Epoch [1/1], Step [5200/6471], Loss: 1.0030, Perplexity: 2.7265\n",
            "Epoch [1/1], Step [5400/6471], Loss: 1.0018, Perplexity: 2.7231\n",
            "Epoch [1/1], Step [5600/6471], Loss: 1.0132, Perplexity: 2.7544\n",
            "Epoch [1/1], Step [5800/6471], Loss: 0.9478, Perplexity: 2.5801\n",
            "Epoch [1/1], Step [6000/6471], Loss: 0.9868, Perplexity: 2.6828\n",
            "Epoch [1/1], Step [6200/6471], Loss: 1.0960, Perplexity: 2.9923\n",
            "Epoch [1/1], Step [6400/6471], Loss: 0.9558, Perplexity: 2.6007\n",
            "Epoch [1/1], Step [6471/6471], Loss: 1.0256, Perplexity: 2.7887"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predicted, true = test(device, image_model, test_dataloader, loss_fn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgorKlyNQYF1",
        "outputId": "b48ad2c3-7c87-4855-f8d1-c4fa5d0b80f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test Error:  Accuracy: 42.7%, Avg loss: 1.081796 \n",
            "\n"
          ]
        }
      ]
    }
  ]
}